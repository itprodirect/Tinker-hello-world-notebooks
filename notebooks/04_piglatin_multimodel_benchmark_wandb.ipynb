{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f67ea6-93ad-415e-a104-d1bee4fc3653",
   "metadata": {},
   "source": [
    "# Notebook 04 — Pig Latin LoRA Multi-Model Benchmark (Tinker + W&B)\n",
    "\n",
    "**Goal:** Compare multiple base models on the same dataset and training setup, tracking:\n",
    "- training loss curves\n",
    "- wall time / step time\n",
    "- a simple quality metric (exact match on a small validation set)\n",
    "- a simple “cost proxy” (completion tokens per epoch)\n",
    "\n",
    "**Models tested:**\n",
    "- meta-llama/Llama-3.2-3B\n",
    "- Qwen/Qwen3-4B-Instruct-2507\n",
    "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
    "\n",
    "**Workflow:**\n",
    "1) Load environment + initialize Tinker client  \n",
    "2) Load dataset + sanity check  \n",
    "3) Define experiment matrix  \n",
    "4) Define helpers + training/eval function  \n",
    "5) Run a single smoke test  \n",
    "6) Run the full matrix + log to W&B (optional)\n",
    "\n",
    "This notebook can run in REPORT MODE without retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e580d4e-c764-401b-8b37-cea3a5167296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import tinker\n",
    "from tinker import types\n",
    "\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734b6b0-c42b-43d9-8a59-edb336988d4f",
   "metadata": {},
   "source": [
    "## 1) Environment + Tinker client\n",
    "\n",
    "We explicitly load `.env` from the repo root (not the current notebook folder), then create a single `service_client`.\n",
    "This avoids “API key not found” and avoids re-creating sessions repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f6399f5-dd4f-4bfe-87a8-2e793765c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: C:\\Users\\user\\Desktop\\tinker-hello-world\n",
      ".env exists: True\n",
      "TINKER_API_KEY present: True\n",
      "TINKER_API_KEY startswith: tml-CD\n",
      "ServiceClient ready ✅\n"
     ]
    }
   ],
   "source": [
    "def find_repo_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \".git\").exists() or (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "ENV_PATH = REPO_ROOT / \".env\"\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\".env exists:\", ENV_PATH.exists())\n",
    "\n",
    "load_dotenv(dotenv_path=ENV_PATH, override=True)\n",
    "\n",
    "api_key = os.getenv(\"TINKER_API_KEY\")\n",
    "print(\"TINKER_API_KEY present:\", bool(api_key))\n",
    "print(\"TINKER_API_KEY startswith:\", (api_key or \"\")[:6])\n",
    "\n",
    "assert api_key, \"Missing TINKER_API_KEY. Put it in repo-root .env as TINKER_API_KEY=...\"\n",
    "\n",
    "service_client = tinker.ServiceClient(api_key=api_key)\n",
    "print(\"ServiceClient ready ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299124e6-0c5f-4bb2-a5b0-0fec0ef3d960",
   "metadata": {},
   "source": [
    "## Tiny guard to prevent training run duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c537c6f-1eea-46ed-8c95-08282dce8978",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRAINING = False   # <- keep False for demo/report work\n",
    "RUN_ANALYSIS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3a163-658f-4441-a9d6-1a3c2e18c8bf",
   "metadata": {},
   "source": [
    "## 2) Dataset\n",
    "\n",
    "We load `data/piglatin/sample.jsonl` and verify it contains `{ \"input\": ..., \"output\": ... }` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c44ca890-bfd4-4909-8b9b-c429340f4f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "Exists: True\n",
      "Rows: 300\n",
      "First row keys: dict_keys(['input', 'output'])\n",
      "First row sample: {'input': 'Translate this to Pig Latin:\\nHaving assembled the senate, he reminded them of the injustice of his', 'output': 'aving-Hay assembled-ay e-thay enate-say, e-hay eminded-ray em-thay of-ay e-thay injustice-ay of-ay is-hay'}\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = REPO_ROOT / \"data\" / \"piglatin\" / \"sample.jsonl\"\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n",
    "print(\"Exists:\", DATA_PATH.exists())\n",
    "assert DATA_PATH.exists(), f\"Missing dataset at {DATA_PATH}\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    path = Path(path)\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "rows = read_jsonl(DATA_PATH)\n",
    "print(\"Rows:\", len(rows))\n",
    "print(\"First row keys:\", rows[0].keys())\n",
    "print(\"First row sample:\", rows[0])\n",
    "\n",
    "assert \"input\" in rows[0] and \"output\" in rows[0], \"Dataset rows must contain 'input' and 'output'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba49a7-8f93-4b50-b7bd-4c4331338c20",
   "metadata": {},
   "source": [
    "## 3) Experiment matrix\n",
    "\n",
    "We define:\n",
    "- which base models to test\n",
    "- training presets (light vs baseline)\n",
    "- safe overrides for the 30B model\n",
    "- a run naming scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18508cc5-a9c5-42ae-b69b-178dc59b647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned runs:\n",
      " - meta-llama/Llama-3.2-3B light\n",
      " - Qwen/Qwen3-4B-Instruct-2507 light\n",
      " - Qwen/Qwen3-30B-A3B-Instruct-2507 light\n",
      " - meta-llama/Llama-3.2-3B baseline\n",
      " - Qwen/Qwen3-4B-Instruct-2507 baseline\n"
     ]
    }
   ],
   "source": [
    "WANDB_PROJECT = \"tinker-hello-world\"\n",
    "WANDB_ENTITY = \"itprodirect\"\n",
    "WANDB_MODE = \"online\"        # use \"offline\" if you want\n",
    "WANDB_GROUP = \"piglatin-multimodel-benchmark-v1\"\n",
    "USE_WANDB = False            # keep False for smoke test; turn True later\n",
    "\n",
    "BASE_MODELS = [\n",
    "    \"meta-llama/Llama-3.2-3B\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    "]\n",
    "\n",
    "PRESETS = {\n",
    "    \"light\": dict(max_train_rows=120, lora_rank=8,  lora_alpha=16, learning_rate=2e-4, num_epochs=2, batch_size=8),\n",
    "    \"baseline\": dict(max_train_rows=300, lora_rank=16, lora_alpha=32, learning_rate=1e-4, num_epochs=3, batch_size=8),\n",
    "}\n",
    "\n",
    "MODEL_OVERRIDES = {\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": dict(batch_size=4, max_train_rows=120),\n",
    "}\n",
    "\n",
    "EXPERIMENT_MATRIX = []\n",
    "for m in BASE_MODELS:\n",
    "    EXPERIMENT_MATRIX.append((m, \"light\"))\n",
    "for m in [\"meta-llama/Llama-3.2-3B\", \"Qwen/Qwen3-4B-Instruct-2507\"]:\n",
    "    EXPERIMENT_MATRIX.append((m, \"baseline\"))\n",
    "\n",
    "def make_run_name(base_model: str, preset: str) -> str:\n",
    "    short = base_model.split(\"/\")[-1]\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"piglatin-{preset}-{short}-{ts}\"\n",
    "\n",
    "print(\"Planned runs:\")\n",
    "for m, p in EXPERIMENT_MATRIX:\n",
    "    print(\" -\", m, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1817daa-741b-4ce3-88d5-e0c0c92e1254",
   "metadata": {},
   "source": [
    "4) Training example format\n",
    "\n",
    "We train next-token prediction where:\n",
    "\n",
    "- prompt tokens are weighted **0**\n",
    "- completion tokens are weighted **1**\n",
    "\n",
    "This forces the loss to focus on the Pig Latin answer portion (not the prompt text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb122d0-4c5f-45ca-9209-48a2467c7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Training example format (prompt weight=0, completion weight=1)\n",
    "\n",
    "from tinker import types\n",
    "import numpy as np\n",
    "\n",
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    \"\"\"\n",
    "    Build a next-token prediction datum:\n",
    "    - Prompt tokens are weight 0 (no loss)\n",
    "    - Completion tokens are weight 1 (loss)\n",
    "    \"\"\"\n",
    "    prompt = f\"English: {example['input']}\\nPig Latin:\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "\n",
    "    completion_tokens = tokenizer.encode(f\" {example['output']}\\n\", add_special_tokens=False)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    "\n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "    weights = weights[1:]  # align with targets\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(weights=weights, target_tokens=target_tokens),\n",
    "    )\n",
    "\n",
    "def _as_1d(x) -> np.ndarray:\n",
    "    \"\"\"Force scalars / tuples / lists / arrays into a 1D numpy array.\"\"\"\n",
    "    if hasattr(x, \"tolist\"):\n",
    "        x = x.tolist()\n",
    "    arr = np.asarray(x)\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    \"\"\"Normalize strings for exact-match comparisons.\"\"\"\n",
    "    return \" \".join(s.strip().lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017d580-0ac4-4bb9-81ef-50de9304d04a",
   "metadata": {},
   "source": [
    "## 5) run_one() — train + evaluate one configuration\n",
    "\n",
    "This cell defines `run_one()` only. It intentionally **does not** overwrite the experiment-matrix globals (WANDB_*, PRESETS, MODEL_OVERRIDES, etc.).\n",
    "\n",
    "Key design decisions:\n",
    "- `data_path=None` (defaults to repo `data/piglatin/sample.jsonl`)\n",
    "- cost proxy is computed from completion-token weights (not Tinker internals)\n",
    "- W&B logging is controlled by your global `USE_WANDB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b8ad91-6fb1-4398-be34-df6fdf12373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 5) Core training + eval\n",
    "# =========================\n",
    "\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tinker\n",
    "from tinker import types\n",
    "\n",
    "# Optional: W&B\n",
    "try:\n",
    "    import wandb\n",
    "except Exception:\n",
    "    wandb = None\n",
    "\n",
    "# Optional: dotenv (loads TINKER_API_KEY if you use a .env file)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "\n",
    "\n",
    "# ---------- Paths / env ----------\n",
    "def find_repo_root(start: str | Path | None = None) -> Path:\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \".git\").exists() or (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "ENV_PATH = REPO_ROOT / \".env\"\n",
    "\n",
    "if load_dotenv is not None and ENV_PATH.exists():\n",
    "    load_dotenv(dotenv_path=ENV_PATH, override=True)\n",
    "\n",
    "DEFAULT_DATA_PATH = str(REPO_ROOT / \"data\" / \"piglatin\" / \"sample.jsonl\")\n",
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def read_jsonl(path: str | Path) -> list[dict]:\n",
    "    path = Path(path)\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _as_1d(x) -> np.ndarray:\n",
    "    \"\"\"Force scalars / tuples / lists / arrays into a 1D numpy array.\"\"\"\n",
    "    if hasattr(x, \"tolist\"):\n",
    "        x = x.tolist()\n",
    "    arr = np.asarray(x)\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "\n",
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    \"\"\"\n",
    "    Next-token prediction datum:\n",
    "    - prompt tokens weight 0 (no loss)\n",
    "    - completion tokens weight 1 (loss)\n",
    "    \"\"\"\n",
    "    prompt = f\"English: {example['input']}\\nPig Latin:\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "\n",
    "    completion_tokens = tokenizer.encode(f\" {example['output']}\\n\", add_special_tokens=False)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    "\n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "    weights = weights[1:]  # align with targets\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(weights=weights, target_tokens=target_tokens),\n",
    "    )\n",
    "\n",
    "\n",
    "def _get_global(name: str, default):\n",
    "    return globals().get(name, default)\n",
    "\n",
    "\n",
    "def run_one(\n",
    "    base_model: str,\n",
    "    preset_name: str,\n",
    "    data_path: str | None = None,\n",
    "    seed: int = 7,\n",
    "    val_rows: int = 25,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train + evaluate exactly one configuration.\n",
    "    Uses experiment globals if present:\n",
    "      WANDB_PROJECT, WANDB_ENTITY, WANDB_MODE, WANDB_GROUP, USE_WANDB\n",
    "      PRESETS, MODEL_OVERRIDES, make_run_name\n",
    "    \"\"\"\n",
    "    # ---- Required globals from earlier sections ----\n",
    "    PRESETS = _get_global(\"PRESETS\", None)\n",
    "    if PRESETS is None or preset_name not in PRESETS:\n",
    "        raise ValueError(\"Missing PRESETS or preset_name not found. Run Section 3 (Experiment matrix) first.\")\n",
    "\n",
    "    MODEL_OVERRIDES = _get_global(\"MODEL_OVERRIDES\", {})\n",
    "    make_run_name = _get_global(\"make_run_name\", None)\n",
    "\n",
    "    # ---- Data path ----\n",
    "    if data_path is None:\n",
    "        data_path = _get_global(\"DATA_PATH\", DEFAULT_DATA_PATH)\n",
    "\n",
    "    # ---- W&B globals ----\n",
    "    WANDB_PROJECT = _get_global(\"WANDB_PROJECT\", \"tinker-hello-world\")\n",
    "    WANDB_ENTITY  = _get_global(\"WANDB_ENTITY\", None)\n",
    "    WANDB_MODE    = _get_global(\"WANDB_MODE\", \"online\")\n",
    "    WANDB_GROUP   = _get_global(\"WANDB_GROUP\", \"piglatin-multimodel-benchmark-v1\")\n",
    "    USE_WANDB     = _get_global(\"USE_WANDB\", False)\n",
    "\n",
    "    # ---- Client (reuse if you already created one earlier) ----\n",
    "    service_client = _get_global(\"service_client\", None) or tinker.ServiceClient()\n",
    "\n",
    "    # ---- Config ----\n",
    "    cfg = dict(PRESETS[preset_name])\n",
    "    cfg.update(MODEL_OVERRIDES.get(base_model, {}))\n",
    "\n",
    "    if make_run_name is None:\n",
    "        def make_run_name(bm: str, ps: str) -> str:\n",
    "            short = bm.split(\"/\")[-1]\n",
    "            ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            return f\"piglatin-{ps}-{short}-{ts}\"\n",
    "\n",
    "    run_name = make_run_name(base_model, preset_name)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    examples = read_jsonl(data_path)[: cfg[\"max_train_rows\"]]\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    val = examples[:val_rows]\n",
    "    train = examples[val_rows:]\n",
    "    assert len(train) > 0, \"Train split is empty; increase max_train_rows or reduce val_rows.\"\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"RUN:\", run_name)\n",
    "    print(\"MODEL:\", base_model)\n",
    "    print(\"CFG:\", cfg)\n",
    "    print(\"DATA:\", data_path)\n",
    "    print(\"USE_WANDB:\", USE_WANDB)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # ---- W&B init (optional) ----\n",
    "    wandb_run = None\n",
    "    if USE_WANDB and wandb is not None:\n",
    "        wandb_run = wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_ENTITY,\n",
    "            mode=WANDB_MODE,\n",
    "            name=run_name,\n",
    "            group=WANDB_GROUP,\n",
    "            tags=[preset_name, base_model.split(\"/\")[-1]],\n",
    "            config=dict(\n",
    "                base_model=base_model,\n",
    "                preset=preset_name,\n",
    "                **cfg,\n",
    "                seed=seed,\n",
    "                train_rows=len(train),\n",
    "                val_rows=len(val),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # ---- Training client + tokenizer ----\n",
    "    training_client = service_client.create_lora_training_client(\n",
    "        base_model=base_model,\n",
    "        rank=cfg[\"lora_rank\"],\n",
    "    )\n",
    "    tokenizer = training_client.get_tokenizer()\n",
    "\n",
    "    processed_train = [process_example(ex, tokenizer) for ex in train]\n",
    "\n",
    "    # Cost proxy (weighted tokens == completion tokens)\n",
    "    completion_tokens_per_epoch = int(\n",
    "        sum(_as_1d(d.loss_fn_inputs[\"weights\"]).sum() for d in processed_train)\n",
    "    )\n",
    "\n",
    "    num_examples = len(processed_train)\n",
    "    steps_per_epoch = int(np.ceil(num_examples / cfg[\"batch_size\"]))\n",
    "    total_steps = cfg[\"num_epochs\"] * steps_per_epoch\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    global_step = 0\n",
    "    step_times = []\n",
    "\n",
    "    for epoch in range(cfg[\"num_epochs\"]):\n",
    "        random.shuffle(processed_train)\n",
    "\n",
    "        for start in range(0, num_examples, cfg[\"batch_size\"]):\n",
    "            batch = processed_train[start : start + cfg[\"batch_size\"]]\n",
    "\n",
    "            st = time.perf_counter()\n",
    "\n",
    "            fwdbwd_future = training_client.forward_backward(batch, loss_fn=\"cross_entropy\")\n",
    "            optim_future = training_client.optim_step(types.AdamParams(learning_rate=cfg[\"learning_rate\"]))\n",
    "\n",
    "            fwdbwd_result = fwdbwd_future.result()\n",
    "            optim_future.result()\n",
    "\n",
    "            logprobs = np.concatenate([_as_1d(out[\"logprobs\"]) for out in fwdbwd_result.loss_fn_outputs])\n",
    "            weights  = np.concatenate([_as_1d(ex.loss_fn_inputs[\"weights\"]) for ex in batch])\n",
    "\n",
    "            if logprobs.shape[0] != weights.shape[0]:\n",
    "                raise ValueError(f\"Length mismatch: logprobs={logprobs.shape} weights={weights.shape}\")\n",
    "\n",
    "            loss = -float(np.dot(logprobs, weights) / weights.sum())\n",
    "\n",
    "            global_step += 1\n",
    "            dt = time.perf_counter() - st\n",
    "            step_times.append(dt)\n",
    "\n",
    "            if USE_WANDB and wandb_run is not None:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss\": loss,\n",
    "                        \"train/epoch\": epoch + 1,\n",
    "                        \"train/step\": global_step,\n",
    "                        \"perf/step_time_s\": dt,\n",
    "                        \"perf/completion_tokens_per_epoch\": completion_tokens_per_epoch,\n",
    "                        \"perf/est_completion_tokens_total\": completion_tokens_per_epoch * cfg[\"num_epochs\"],\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{cfg['num_epochs']} Step {global_step}/{total_steps} Loss {loss:.4f} StepTime {dt:.2f}s\")\n",
    "\n",
    "    wall_time_s = time.perf_counter() - t0\n",
    "    avg_step_time_s = float(np.mean(step_times)) if step_times else None\n",
    "\n",
    "    # Save adapter + sampling client\n",
    "    sampling_client = training_client.save_weights_and_get_sampling_client(name=run_name)\n",
    "\n",
    "    # ---- Eval ----\n",
    "    params = types.SamplingParams(max_tokens=60, temperature=0.0, top_p=1.0)\n",
    "\n",
    "    correct = 0\n",
    "    rows_out = []\n",
    "\n",
    "    for ex in val:\n",
    "        prompt = f\"English: {ex['input']}\\nPig Latin:\"\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        model_input = types.ModelInput.from_ints(tokens=prompt_tokens)\n",
    "\n",
    "        result = sampling_client.sample(\n",
    "            prompt=model_input,\n",
    "            sampling_params=params,\n",
    "            num_samples=1\n",
    "        ).result()\n",
    "\n",
    "        seq_tokens = result.sequences[0].tokens\n",
    "\n",
    "        # Strip prompt prefix if present\n",
    "        if len(seq_tokens) >= len(prompt_tokens) and seq_tokens[:len(prompt_tokens)] == prompt_tokens:\n",
    "            new_tokens = seq_tokens[len(prompt_tokens):]\n",
    "        else:\n",
    "            new_tokens = seq_tokens\n",
    "\n",
    "        decoded_new = tokenizer.decode(new_tokens).strip()\n",
    "        pred = decoded_new.splitlines()[0].strip() if decoded_new else \"\"\n",
    "\n",
    "        gold = ex[\"output\"].strip()\n",
    "        is_ok = (norm(pred) == norm(gold))\n",
    "\n",
    "        correct += int(is_ok)\n",
    "        rows_out.append([ex[\"input\"], gold, pred, is_ok])\n",
    "\n",
    "        # Print first 5 samples for sanity\n",
    "        if len(rows_out) <= 5:\n",
    "            print(\"----\")\n",
    "            print(\"IN  :\", ex[\"input\"])\n",
    "            print(\"GOLD:\", gold)\n",
    "            print(\"PRED:\", pred if pred else \"<EMPTY>\")\n",
    "            print(\"OK? :\", is_ok)\n",
    "\n",
    "    exact_match = correct / len(val) if val else 0.0\n",
    "\n",
    "    if USE_WANDB and wandb_run is not None:\n",
    "        table = wandb.Table(columns=[\"input\", \"expected\", \"pred\", \"ok\"], data=rows_out[:10])\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"eval/exact_match_norm\": exact_match,\n",
    "                \"eval/samples\": table,\n",
    "                \"perf/wall_time_s\": wall_time_s,\n",
    "                \"perf/avg_step_time_s\": avg_step_time_s,\n",
    "            }\n",
    "        )\n",
    "        wandb_run.summary[\"perf/wall_time_s\"] = wall_time_s\n",
    "        wandb_run.summary[\"eval/exact_match_norm\"] = exact_match\n",
    "        wandb.finish()\n",
    "\n",
    "    print(f\"Done. wall_time_s={wall_time_s:.1f} eval_exact_match_norm={exact_match:.3f}\")\n",
    "    return {\n",
    "        \"run_name\": run_name,\n",
    "        \"base_model\": base_model,\n",
    "        \"preset\": preset_name,\n",
    "        \"wall_time_s\": wall_time_s,\n",
    "        \"avg_step_time_s\": avg_step_time_s,\n",
    "        \"exact_match_norm\": exact_match,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9a05c-fd2c-4ba5-b936-09e64b0d36a0",
   "metadata": {},
   "source": [
    "## 6) Smoke test\n",
    "\n",
    "Run exactly one small experiment (**no W&B**) to verify:\n",
    "- dataset loads\n",
    "- training loop runs\n",
    "- adapter saves\n",
    "- evaluation produces a metric\n",
    "\n",
    "If this passes, we can confidently run the full matrix with W&B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3f0268-e048-47b2-8b31-c9b2e8df9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Llama-3.2-3B-20251212_122829\n",
      "MODEL: meta-llama/Llama-3.2-3B\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "USE_WANDB: False\n",
      "==============================\n",
      "Epoch 1/2 Step 1/28 Loss 3.2360 StepTime 1.11s\n",
      "Epoch 1/2 Step 2/28 Loss 2.8872 StepTime 1.19s\n",
      "Epoch 1/2 Step 3/28 Loss 2.4037 StepTime 1.19s\n",
      "Epoch 1/2 Step 4/28 Loss 1.9843 StepTime 1.26s\n",
      "Epoch 1/2 Step 5/28 Loss 1.6920 StepTime 1.18s\n",
      "Epoch 1/2 Step 6/28 Loss 1.4705 StepTime 1.12s\n",
      "Epoch 1/2 Step 7/28 Loss 1.3142 StepTime 1.18s\n",
      "Epoch 1/2 Step 8/28 Loss 1.1682 StepTime 1.17s\n",
      "Epoch 1/2 Step 9/28 Loss 1.0955 StepTime 1.22s\n",
      "Epoch 1/2 Step 10/28 Loss 0.9103 StepTime 1.28s\n",
      "Epoch 1/2 Step 11/28 Loss 0.8582 StepTime 1.32s\n",
      "Epoch 1/2 Step 12/28 Loss 0.7154 StepTime 1.21s\n",
      "Epoch 1/2 Step 13/28 Loss 0.5864 StepTime 1.26s\n",
      "Epoch 1/2 Step 14/28 Loss 0.6813 StepTime 1.23s\n",
      "Epoch 2/2 Step 15/28 Loss 0.5003 StepTime 1.22s\n",
      "Epoch 2/2 Step 16/28 Loss 0.3684 StepTime 1.22s\n",
      "Epoch 2/2 Step 17/28 Loss 0.2469 StepTime 1.30s\n",
      "Epoch 2/2 Step 18/28 Loss 0.2622 StepTime 1.17s\n",
      "Epoch 2/2 Step 19/28 Loss 0.2406 StepTime 1.38s\n",
      "Epoch 2/2 Step 20/28 Loss 0.2579 StepTime 1.08s\n",
      "Epoch 2/2 Step 21/28 Loss 0.2769 StepTime 1.21s\n",
      "Epoch 2/2 Step 22/28 Loss 0.1355 StepTime 1.29s\n",
      "Epoch 2/2 Step 23/28 Loss 0.3244 StepTime 1.37s\n",
      "Epoch 2/2 Step 24/28 Loss 0.2298 StepTime 1.17s\n",
      "Epoch 2/2 Step 25/28 Loss 0.1610 StepTime 1.18s\n",
      "Epoch 2/2 Step 26/28 Loss 0.1854 StepTime 1.28s\n",
      "Epoch 2/2 Step 27/28 Loss 0.2417 StepTime 1.17s\n",
      "Epoch 2/2 Step 28/28 Loss 0.2262 StepTime 1.20s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-Gay elgica-Bay, ont-hay o-tay ave-hay ewelt-day in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: try-ay ever-whay ortune-fay ay all-fay oplay all-fay o-tay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: usual-ay, o-tay etray e-thay orfagers-fay. ith-Whay ese-thay e-hay intermixe-day a-ay uard-gay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oint-jay iridovix-Vay, G. ellow-iii. 17; aulerci-Aulerci annovices-Brannovices ordered-ordered o-tay urnish-fay ihr-thay\n",
      "OK? : False\n",
      "Done. wall_time_s=34.2 eval_exact_match_norm=0.200\n"
     ]
    }
   ],
   "source": [
    "RUN_SMOKE = True\n",
    "\n",
    "if RUN_SMOKE:\n",
    "    USE_WANDB = False  # force off for smoke test\n",
    "\n",
    "    smoke_result = run_one(\n",
    "        base_model=\"meta-llama/Llama-3.2-3B\",\n",
    "        preset_name=\"light\",\n",
    "        val_rows=10,   # faster\n",
    "        seed=7,\n",
    "    )\n",
    "\n",
    "    smoke_result\n",
    "else:\n",
    "    print(\"Skipping smoke test (RUN_SMOKE=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb08f8-d8ad-412c-8208-a43ad14f00ac",
   "metadata": {},
   "source": [
    "## 7) Full experiment matrix (with W&B)\n",
    "\n",
    "Now we run every (model, preset) in `EXPERIMENT_MATRIX` and log to Weights & Biases.\n",
    "\n",
    "Tip: keep an eye on the 30B run — it uses safer overrides by default.\n",
    "\n",
    "- evaluation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe04d53b-b45c-42c1-b3b1-f34b217c389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### RUNNING: meta-llama/Llama-3.2-3B light\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Llama-3.2-3B-20251212_124034\n",
      "MODEL: meta-llama/Llama-3.2-3B\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "USE_WANDB: True\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: nick99 (itprodirect) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_124038-8byuow5e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/8byuow5e' target=\"_blank\">piglatin-light-Llama-3.2-3B-20251212_124034</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/8byuow5e' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/8byuow5e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Step 1/24 Loss 3.2464 StepTime 1.21s\n",
      "Epoch 1/2 Step 2/24 Loss 2.9429 StepTime 1.22s\n",
      "Epoch 1/2 Step 3/24 Loss 2.3487 StepTime 1.20s\n",
      "Epoch 1/2 Step 4/24 Loss 1.9152 StepTime 1.14s\n",
      "Epoch 1/2 Step 5/24 Loss 1.7097 StepTime 1.25s\n",
      "Epoch 1/2 Step 6/24 Loss 1.5891 StepTime 1.24s\n",
      "Epoch 1/2 Step 7/24 Loss 1.3890 StepTime 1.08s\n",
      "Epoch 1/2 Step 8/24 Loss 1.0961 StepTime 1.09s\n",
      "Epoch 1/2 Step 9/24 Loss 1.0075 StepTime 1.18s\n",
      "Epoch 1/2 Step 10/24 Loss 0.8410 StepTime 1.19s\n",
      "Epoch 1/2 Step 11/24 Loss 0.5656 StepTime 1.17s\n",
      "Epoch 1/2 Step 12/24 Loss 0.5703 StepTime 1.19s\n",
      "Epoch 2/2 Step 13/24 Loss 0.4717 StepTime 1.25s\n",
      "Epoch 2/2 Step 14/24 Loss 0.3353 StepTime 1.17s\n",
      "Epoch 2/2 Step 15/24 Loss 0.3556 StepTime 1.19s\n",
      "Epoch 2/2 Step 16/24 Loss 0.3409 StepTime 1.23s\n",
      "Epoch 2/2 Step 17/24 Loss 0.2117 StepTime 1.19s\n",
      "Epoch 2/2 Step 18/24 Loss 0.4053 StepTime 1.33s\n",
      "Epoch 2/2 Step 19/24 Loss 0.3042 StepTime 1.18s\n",
      "Epoch 2/2 Step 20/24 Loss 0.4137 StepTime 1.18s\n",
      "Epoch 2/2 Step 21/24 Loss 0.3117 StepTime 1.11s\n",
      "Epoch 2/2 Step 22/24 Loss 0.2126 StepTime 1.18s\n",
      "Epoch 2/2 Step 23/24 Loss 0.2617 StepTime 1.19s\n",
      "Epoch 2/2 Step 24/24 Loss 0.1862 StepTime 1.18s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-Nay, an-ay ancient-ay eople- pay of-ay _Gallia-ay elgica-Bay, oth-thay o-tay ave-hay id-way ent-way in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: y-tray atever-whay ortune-fay ay-fallay o-tay eir-thay o-tay. I-f-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-cay ot-nay e-bay ett-ay ack-bay y-bay e-thay uburnes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: ual-say, o-tay oprotect-pay e-thay or-ayers-fray. ith-Whay ese-thay es-hay intermixed-ay a-ay uard-gay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oin-jay iridovix-ay, G. iii-ay. 17; ulerci-Brannovices orded-ray o-tay vantage-fur; y-thay\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▅▅▄▃▆▆▁▁▄▄▄▄▆▄▄▅▄█▄▄▂▄▄▄</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>train/loss</td><td>█▇▆▅▄▄▄▃▃▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0</td></tr><tr><td>perf/avg_step_time_s</td><td>1.18844</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>3855</td></tr><tr><td>perf/est_completion_tokens_total</td><td>7710</td></tr><tr><td>perf/step_time_s</td><td>1.18167</td></tr><tr><td>perf/wall_time_s</td><td>28.62044</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>0.18619</td></tr><tr><td>train/step</td><td>24</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-light-Llama-3.2-3B-20251212_124034</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/8byuow5e' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/8byuow5e</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_124038-8byuow5e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=28.6 eval_exact_match_norm=0.000\n",
      "\n",
      "\n",
      "### RUNNING: Qwen/Qwen3-4B-Instruct-2507 light\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Qwen3-4B-Instruct-2507-20251212_124208\n",
      "MODEL: Qwen/Qwen3-4B-Instruct-2507\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "USE_WANDB: True\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_124208-g76cpwcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/g76cpwcz' target=\"_blank\">piglatin-light-Qwen3-4B-Instruct-2507-20251212_124208</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/g76cpwcz' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/g76cpwcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Step 1/24 Loss 2.3160 StepTime 1.71s\n",
      "Epoch 1/2 Step 2/24 Loss 2.4483 StepTime 1.66s\n",
      "Epoch 1/2 Step 3/24 Loss 1.4622 StepTime 1.48s\n",
      "Epoch 1/2 Step 4/24 Loss 1.2635 StepTime 1.72s\n",
      "Epoch 1/2 Step 5/24 Loss 0.9200 StepTime 1.49s\n",
      "Epoch 1/2 Step 6/24 Loss 0.7974 StepTime 1.53s\n",
      "Epoch 1/2 Step 7/24 Loss 0.7069 StepTime 1.54s\n",
      "Epoch 1/2 Step 8/24 Loss 0.4907 StepTime 1.62s\n",
      "Epoch 1/2 Step 9/24 Loss 0.5689 StepTime 1.47s\n",
      "Epoch 1/2 Step 10/24 Loss 0.4895 StepTime 1.47s\n",
      "Epoch 1/2 Step 11/24 Loss 0.2038 StepTime 1.49s\n",
      "Epoch 1/2 Step 12/24 Loss 0.1711 StepTime 1.48s\n",
      "Epoch 2/2 Step 13/24 Loss 0.1830 StepTime 1.46s\n",
      "Epoch 2/2 Step 14/24 Loss 0.2119 StepTime 1.44s\n",
      "Epoch 2/2 Step 15/24 Loss 0.1275 StepTime 1.49s\n",
      "Epoch 2/2 Step 16/24 Loss 0.1285 StepTime 1.51s\n",
      "Epoch 2/2 Step 17/24 Loss 0.0901 StepTime 1.46s\n",
      "Epoch 2/2 Step 18/24 Loss 0.1498 StepTime 1.47s\n",
      "Epoch 2/2 Step 19/24 Loss 0.1804 StepTime 1.75s\n",
      "Epoch 2/2 Step 20/24 Loss 0.3026 StepTime 1.53s\n",
      "Epoch 2/2 Step 21/24 Loss 0.2039 StepTime 1.55s\n",
      "Epoch 2/2 Step 22/24 Loss 0.0861 StepTime 1.49s\n",
      "Epoch 2/2 Step 23/24 Loss 0.2030 StepTime 1.45s\n",
      "Epoch 2/2 Step 24/24 Loss 0.1330 StepTime 1.75s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-ay, a-nay antic-ay eople-pay of-ay _allia-Gay elgica-Bay_, ought-thay o-tay ave-hay olt-ed-day in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: y-tray atever-whay untance-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-cay ot-nay e-bay ept-ky ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: ual-say, o-tay otect-pray e-thay oragers-fay. ith-way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oin-jay iroday-hidovanix, Gay. iii. 17; Aureci-ay annovices-bray ordered-ay o-tay iversay\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▇▆▂▇▂▃▃▅▂▂▂▂▁▁▂▃▂▂█▃▃▂▁█</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>train/loss</td><td>██▅▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.08</td></tr><tr><td>perf/avg_step_time_s</td><td>1.54227</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>3862</td></tr><tr><td>perf/est_completion_tokens_total</td><td>7724</td></tr><tr><td>perf/step_time_s</td><td>1.75148</td></tr><tr><td>perf/wall_time_s</td><td>37.11905</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>0.13302</td></tr><tr><td>train/step</td><td>24</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-light-Qwen3-4B-Instruct-2507-20251212_124208</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/g76cpwcz' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/g76cpwcz</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_124208-g76cpwcz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=37.1 eval_exact_match_norm=0.080\n",
      "\n",
      "\n",
      "### RUNNING: Qwen/Qwen3-30B-A3B-Instruct-2507 light\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Qwen3-30B-A3B-Instruct-2507-20251212_124350\n",
      "MODEL: Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 4}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "USE_WANDB: True\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_124350-ka1ke3sz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/ka1ke3sz' target=\"_blank\">piglatin-light-Qwen3-30B-A3B-Instruct-2507-20251212_124350</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/ka1ke3sz' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/ka1ke3sz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Step 1/48 Loss 0.7406 StepTime 2.34s\n",
      "Epoch 1/2 Step 2/48 Loss 0.7420 StepTime 3.28s\n",
      "Epoch 1/2 Step 3/48 Loss 0.4474 StepTime 3.15s\n",
      "Epoch 1/2 Step 4/48 Loss 0.8899 StepTime 3.16s\n",
      "Epoch 1/2 Step 5/48 Loss 0.4542 StepTime 2.62s\n",
      "Epoch 1/2 Step 6/48 Loss 0.1545 StepTime 2.08s\n",
      "Epoch 1/2 Step 7/48 Loss 0.3458 StepTime 1.99s\n",
      "Epoch 1/2 Step 8/48 Loss 0.3086 StepTime 2.20s\n",
      "Epoch 1/2 Step 9/48 Loss 0.2674 StepTime 2.05s\n",
      "Epoch 1/2 Step 10/48 Loss 0.1756 StepTime 2.01s\n",
      "Epoch 1/2 Step 11/48 Loss 0.1984 StepTime 2.07s\n",
      "Epoch 1/2 Step 12/48 Loss 0.3278 StepTime 2.04s\n",
      "Epoch 1/2 Step 13/48 Loss 0.2864 StepTime 2.00s\n",
      "Epoch 1/2 Step 14/48 Loss 0.2851 StepTime 1.97s\n",
      "Epoch 1/2 Step 15/48 Loss 0.2438 StepTime 2.46s\n",
      "Epoch 1/2 Step 16/48 Loss 0.1169 StepTime 2.00s\n",
      "Epoch 1/2 Step 17/48 Loss 0.1749 StepTime 2.24s\n",
      "Epoch 1/2 Step 18/48 Loss 0.1752 StepTime 2.12s\n",
      "Epoch 1/2 Step 19/48 Loss 0.1128 StepTime 1.99s\n",
      "Epoch 1/2 Step 20/48 Loss 0.1250 StepTime 1.97s\n",
      "Epoch 1/2 Step 21/48 Loss 0.0882 StepTime 2.06s\n",
      "Epoch 1/2 Step 22/48 Loss 0.0542 StepTime 1.98s\n",
      "Epoch 1/2 Step 23/48 Loss 0.0524 StepTime 1.96s\n",
      "Epoch 1/2 Step 24/48 Loss 0.0316 StepTime 2.08s\n",
      "Epoch 2/2 Step 25/48 Loss 0.0251 StepTime 2.05s\n",
      "Epoch 2/2 Step 26/48 Loss 0.0345 StepTime 2.02s\n",
      "Epoch 2/2 Step 27/48 Loss 0.0481 StepTime 2.18s\n",
      "Epoch 2/2 Step 28/48 Loss 0.0382 StepTime 1.99s\n",
      "Epoch 2/2 Step 29/48 Loss 0.0604 StepTime 2.04s\n",
      "Epoch 2/2 Step 30/48 Loss 0.0199 StepTime 2.00s\n",
      "Epoch 2/2 Step 31/48 Loss 0.0522 StepTime 1.98s\n",
      "Epoch 2/2 Step 32/48 Loss 0.0828 StepTime 1.96s\n",
      "Epoch 2/2 Step 33/48 Loss 0.1393 StepTime 2.00s\n",
      "Epoch 2/2 Step 34/48 Loss 0.0149 StepTime 2.08s\n",
      "Epoch 2/2 Step 35/48 Loss 0.0432 StepTime 1.99s\n",
      "Epoch 2/2 Step 36/48 Loss 0.0376 StepTime 2.19s\n",
      "Epoch 2/2 Step 37/48 Loss 0.0102 StepTime 1.86s\n",
      "Epoch 2/2 Step 38/48 Loss 0.0700 StepTime 1.88s\n",
      "Epoch 2/2 Step 39/48 Loss 0.1454 StepTime 2.14s\n",
      "Epoch 2/2 Step 40/48 Loss 0.0658 StepTime 3.12s\n",
      "Epoch 2/2 Step 41/48 Loss 0.0815 StepTime 2.16s\n",
      "Epoch 2/2 Step 42/48 Loss 0.0152 StepTime 2.09s\n",
      "Epoch 2/2 Step 43/48 Loss 0.0101 StepTime 2.19s\n",
      "Epoch 2/2 Step 44/48 Loss 0.0330 StepTime 2.09s\n",
      "Epoch 2/2 Step 45/48 Loss 0.0757 StepTime 2.21s\n",
      "Epoch 2/2 Step 46/48 Loss 0.0218 StepTime 16.18s\n",
      "Epoch 2/2 Step 47/48 Loss 0.0189 StepTime 6.12s\n",
      "Epoch 2/2 Step 48/48 Loss 0.0070 StepTime 8.90s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-Nay, an-ay ancient-ay eople-pay of-ay _Gallia-ay elgica-Bay_, ought-thay o-tay ave-hay ewlt-day in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: y-tray ever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunest-ray of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: usual-ay, o-tay o-tay otay o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oin-jay iridovix-ay, y-G.ay ii-ay.ay 17; ay-ay ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁█▄</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>train/loss</td><td>▇▇▄█▅▄▃▃▂▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▁▁▁▁▂▁▂▁▁▂▁▁</td></tr><tr><td>train/step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.32</td></tr><tr><td>perf/avg_step_time_s</td><td>2.69313</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>3862</td></tr><tr><td>perf/est_completion_tokens_total</td><td>7724</td></tr><tr><td>perf/step_time_s</td><td>8.90231</td></tr><tr><td>perf/wall_time_s</td><td>129.48185</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>0.00704</td></tr><tr><td>train/step</td><td>48</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-light-Qwen3-30B-A3B-Instruct-2507-20251212_124350</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/ka1ke3sz' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/ka1ke3sz</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_124350-ka1ke3sz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=129.5 eval_exact_match_norm=0.320\n",
      "\n",
      "\n",
      "### RUNNING: meta-llama/Llama-3.2-3B baseline\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-baseline-Llama-3.2-3B-20251212_124912\n",
      "MODEL: meta-llama/Llama-3.2-3B\n",
      "CFG: {'max_train_rows': 300, 'lora_rank': 16, 'lora_alpha': 32, 'learning_rate': 0.0001, 'num_epochs': 3, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "USE_WANDB: True\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_124912-fvozjttu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/fvozjttu' target=\"_blank\">piglatin-baseline-Llama-3.2-3B-20251212_124912</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/fvozjttu' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/fvozjttu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Step 1/105 Loss 3.5147 StepTime 1.25s\n",
      "Epoch 1/3 Step 2/105 Loss 3.2140 StepTime 1.27s\n",
      "Epoch 1/3 Step 3/105 Loss 2.8068 StepTime 1.24s\n",
      "Epoch 1/3 Step 4/105 Loss 2.2775 StepTime 1.43s\n",
      "Epoch 1/3 Step 5/105 Loss 2.0735 StepTime 1.25s\n",
      "Epoch 1/3 Step 6/105 Loss 2.0170 StepTime 1.09s\n",
      "Epoch 1/3 Step 7/105 Loss 1.6870 StepTime 1.19s\n",
      "Epoch 1/3 Step 8/105 Loss 1.9437 StepTime 1.24s\n",
      "Epoch 1/3 Step 9/105 Loss 1.6143 StepTime 1.22s\n",
      "Epoch 1/3 Step 10/105 Loss 1.3037 StepTime 1.14s\n",
      "Epoch 1/3 Step 11/105 Loss 1.3167 StepTime 1.05s\n",
      "Epoch 1/3 Step 12/105 Loss 1.0453 StepTime 1.16s\n",
      "Epoch 1/3 Step 13/105 Loss 0.9121 StepTime 1.29s\n",
      "Epoch 1/3 Step 14/105 Loss 0.9772 StepTime 1.17s\n",
      "Epoch 1/3 Step 15/105 Loss 0.8836 StepTime 1.28s\n",
      "Epoch 1/3 Step 16/105 Loss 0.9388 StepTime 1.23s\n",
      "Epoch 1/3 Step 17/105 Loss 0.7614 StepTime 1.25s\n",
      "Epoch 1/3 Step 18/105 Loss 0.6911 StepTime 1.19s\n",
      "Epoch 1/3 Step 19/105 Loss 0.8142 StepTime 1.06s\n",
      "Epoch 1/3 Step 20/105 Loss 0.7479 StepTime 1.16s\n",
      "Epoch 1/3 Step 21/105 Loss 0.6071 StepTime 1.19s\n",
      "Epoch 1/3 Step 22/105 Loss 0.4190 StepTime 1.27s\n",
      "Epoch 1/3 Step 23/105 Loss 0.4325 StepTime 1.25s\n",
      "Epoch 1/3 Step 24/105 Loss 0.4170 StepTime 1.14s\n",
      "Epoch 1/3 Step 25/105 Loss 0.5877 StepTime 1.20s\n",
      "Epoch 1/3 Step 26/105 Loss 0.3858 StepTime 1.18s\n",
      "Epoch 1/3 Step 27/105 Loss 0.2854 StepTime 1.17s\n",
      "Epoch 1/3 Step 28/105 Loss 0.3385 StepTime 1.27s\n",
      "Epoch 1/3 Step 29/105 Loss 0.3499 StepTime 1.14s\n",
      "Epoch 1/3 Step 30/105 Loss 0.2566 StepTime 1.18s\n",
      "Epoch 1/3 Step 31/105 Loss 0.3098 StepTime 1.23s\n",
      "Epoch 1/3 Step 32/105 Loss 0.3553 StepTime 1.16s\n",
      "Epoch 1/3 Step 33/105 Loss 0.3939 StepTime 1.20s\n",
      "Epoch 1/3 Step 34/105 Loss 0.2078 StepTime 1.26s\n",
      "Epoch 1/3 Step 35/105 Loss 0.2212 StepTime 1.09s\n",
      "Epoch 2/3 Step 36/105 Loss 0.1544 StepTime 1.16s\n",
      "Epoch 2/3 Step 37/105 Loss 0.1218 StepTime 1.16s\n",
      "Epoch 2/3 Step 38/105 Loss 0.1613 StepTime 1.19s\n",
      "Epoch 2/3 Step 39/105 Loss 0.2171 StepTime 1.19s\n",
      "Epoch 2/3 Step 40/105 Loss 0.1606 StepTime 1.33s\n",
      "Epoch 2/3 Step 41/105 Loss 0.1165 StepTime 1.18s\n",
      "Epoch 2/3 Step 42/105 Loss 0.2605 StepTime 1.17s\n",
      "Epoch 2/3 Step 43/105 Loss 0.1702 StepTime 1.08s\n",
      "Epoch 2/3 Step 44/105 Loss 0.1691 StepTime 1.06s\n",
      "Epoch 2/3 Step 45/105 Loss 0.1678 StepTime 1.25s\n",
      "Epoch 2/3 Step 46/105 Loss 0.1711 StepTime 1.17s\n",
      "Epoch 2/3 Step 47/105 Loss 0.1162 StepTime 1.38s\n",
      "Epoch 2/3 Step 48/105 Loss 0.1175 StepTime 1.12s\n",
      "Epoch 2/3 Step 49/105 Loss 0.1129 StepTime 1.17s\n",
      "Epoch 2/3 Step 50/105 Loss 0.1422 StepTime 1.19s\n",
      "Epoch 2/3 Step 51/105 Loss 0.1183 StepTime 1.06s\n",
      "Epoch 2/3 Step 52/105 Loss 0.1623 StepTime 1.17s\n",
      "Epoch 2/3 Step 53/105 Loss 0.1317 StepTime 1.24s\n",
      "Epoch 2/3 Step 54/105 Loss 0.1912 StepTime 1.25s\n",
      "Epoch 2/3 Step 55/105 Loss 0.1344 StepTime 1.19s\n",
      "Epoch 2/3 Step 56/105 Loss 0.1964 StepTime 1.17s\n",
      "Epoch 2/3 Step 57/105 Loss 0.1573 StepTime 1.07s\n",
      "Epoch 2/3 Step 58/105 Loss 0.1221 StepTime 1.03s\n",
      "Epoch 2/3 Step 59/105 Loss 0.1291 StepTime 1.24s\n",
      "Epoch 2/3 Step 60/105 Loss 0.1494 StepTime 1.27s\n",
      "Epoch 2/3 Step 61/105 Loss 0.1321 StepTime 1.17s\n",
      "Epoch 2/3 Step 62/105 Loss 0.0956 StepTime 1.10s\n",
      "Epoch 2/3 Step 63/105 Loss 0.1699 StepTime 1.27s\n",
      "Epoch 2/3 Step 64/105 Loss 0.0894 StepTime 1.16s\n",
      "Epoch 2/3 Step 65/105 Loss 0.0658 StepTime 1.20s\n",
      "Epoch 2/3 Step 66/105 Loss 0.1097 StepTime 1.16s\n",
      "Epoch 2/3 Step 67/105 Loss 0.1114 StepTime 1.12s\n",
      "Epoch 2/3 Step 68/105 Loss 0.1418 StepTime 1.19s\n",
      "Epoch 2/3 Step 69/105 Loss 0.1380 StepTime 1.14s\n",
      "Epoch 2/3 Step 70/105 Loss 0.1576 StepTime 1.21s\n",
      "Epoch 3/3 Step 71/105 Loss 0.1255 StepTime 1.27s\n",
      "Epoch 3/3 Step 72/105 Loss 0.0486 StepTime 1.23s\n",
      "Epoch 3/3 Step 73/105 Loss 0.0436 StepTime 1.26s\n",
      "Epoch 3/3 Step 74/105 Loss 0.0609 StepTime 1.17s\n",
      "Epoch 3/3 Step 75/105 Loss 0.0288 StepTime 1.17s\n",
      "Epoch 3/3 Step 76/105 Loss 0.0390 StepTime 1.17s\n",
      "Epoch 3/3 Step 77/105 Loss 0.0396 StepTime 1.08s\n",
      "Epoch 3/3 Step 78/105 Loss 0.0300 StepTime 1.29s\n",
      "Epoch 3/3 Step 79/105 Loss 0.0613 StepTime 1.38s\n",
      "Epoch 3/3 Step 80/105 Loss 0.0376 StepTime 1.28s\n",
      "Epoch 3/3 Step 81/105 Loss 0.0636 StepTime 1.17s\n",
      "Epoch 3/3 Step 82/105 Loss 0.0781 StepTime 1.26s\n",
      "Epoch 3/3 Step 83/105 Loss 0.0434 StepTime 1.08s\n",
      "Epoch 3/3 Step 84/105 Loss 0.0404 StepTime 1.17s\n",
      "Epoch 3/3 Step 85/105 Loss 0.0566 StepTime 1.25s\n",
      "Epoch 3/3 Step 86/105 Loss 0.0800 StepTime 1.14s\n",
      "Epoch 3/3 Step 87/105 Loss 0.0378 StepTime 1.26s\n",
      "Epoch 3/3 Step 88/105 Loss 0.0555 StepTime 1.26s\n",
      "Epoch 3/3 Step 89/105 Loss 0.0348 StepTime 1.20s\n",
      "Epoch 3/3 Step 90/105 Loss 0.0357 StepTime 1.20s\n",
      "Epoch 3/3 Step 91/105 Loss 0.0407 StepTime 1.73s\n",
      "Epoch 3/3 Step 92/105 Loss 0.0656 StepTime 1.23s\n",
      "Epoch 3/3 Step 93/105 Loss 0.0359 StepTime 1.18s\n",
      "Epoch 3/3 Step 94/105 Loss 0.0434 StepTime 1.19s\n",
      "Epoch 3/3 Step 95/105 Loss 0.0867 StepTime 1.18s\n",
      "Epoch 3/3 Step 96/105 Loss 0.0507 StepTime 1.09s\n",
      "Epoch 3/3 Step 97/105 Loss 0.0408 StepTime 1.19s\n",
      "Epoch 3/3 Step 98/105 Loss 0.0456 StepTime 1.57s\n",
      "Epoch 3/3 Step 99/105 Loss 0.0424 StepTime 1.20s\n",
      "Epoch 3/3 Step 100/105 Loss 0.1041 StepTime 1.17s\n",
      "Epoch 3/3 Step 101/105 Loss 0.0790 StepTime 1.27s\n",
      "Epoch 3/3 Step 102/105 Loss 0.1037 StepTime 1.21s\n",
      "Epoch 3/3 Step 103/105 Loss 0.0545 StepTime 1.33s\n",
      "Epoch 3/3 Step 104/105 Loss 0.0314 StepTime 1.15s\n",
      "Epoch 3/3 Step 105/105 Loss 0.0258 StepTime 1.18s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "the vengeance of their enemies. Immediately they ran in crowds to the\n",
      "GOLD: e-thay engeance-vay of-ay eir-thay enemies-ay. ediately-Immay ey-thay an-ray in-ay owds-cray o-tay e-thay\n",
      "PRED: e-thay enrage-vica of-ay eir-thay emies-enay. immediately-ay ey-thay an-ray in-ay ows-cray o-tay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "gates were left open, are crowded together in them; then the Germans\n",
      "GOLD: ates-gay ere-way eft-lay open-ay, are-ay owded-cray ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "PRED: ates-gay ere-way eft-lay open-ay, are-ay ivied-crowed-ray ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "30 sentiments such of his fellow citizens as he has access to: he\n",
      "GOLD: 30 entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "PRED: 30-ay entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay as-hay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "X.--Roscius and Lucius Caesar, having received this message, went to\n",
      "GOLD: oscius-X.--Ray and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "PRED: iscus-X.--Ray, and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, o-tay ent-way\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "border-men. Germans, G. i. 51\n",
      "GOLD: order-men-bay. ermans-Gay, G-ay. i-ay. 51\n",
      "PRED: order-men-bay. ems-Gay, G-ay. i-ay. 51\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▃▅▁▂▃▂▂▃▂▃▂▂▃▃▁▂▂▁▂▂▁▂▃▂▂▂▃▃▂▃▃▃▂█▃▂▂▁▂▂</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train/loss</td><td>█▇▅▅▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.24</td></tr><tr><td>perf/avg_step_time_s</td><td>1.20392</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>11166</td></tr><tr><td>perf/est_completion_tokens_total</td><td>33498</td></tr><tr><td>perf/step_time_s</td><td>1.18149</td></tr><tr><td>perf/wall_time_s</td><td>126.83351</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/loss</td><td>0.02581</td></tr><tr><td>train/step</td><td>105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-baseline-Llama-3.2-3B-20251212_124912</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/fvozjttu' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/fvozjttu</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_124912-fvozjttu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=126.8 eval_exact_match_norm=0.240\n",
      "\n",
      "\n",
      "### RUNNING: Qwen/Qwen3-4B-Instruct-2507 baseline\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-baseline-Qwen3-4B-Instruct-2507-20251212_125218\n",
      "MODEL: Qwen/Qwen3-4B-Instruct-2507\n",
      "CFG: {'max_train_rows': 300, 'lora_rank': 16, 'lora_alpha': 32, 'learning_rate': 0.0001, 'num_epochs': 3, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "USE_WANDB: True\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_125218-yed1fcme</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/yed1fcme' target=\"_blank\">piglatin-baseline-Qwen3-4B-Instruct-2507-20251212_125218</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/yed1fcme' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/yed1fcme</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Step 1/105 Loss 3.1657 StepTime 1.54s\n",
      "Epoch 1/3 Step 2/105 Loss 2.2801 StepTime 1.61s\n",
      "Epoch 1/3 Step 3/105 Loss 1.8703 StepTime 1.49s\n",
      "Epoch 1/3 Step 4/105 Loss 1.4698 StepTime 1.48s\n",
      "Epoch 1/3 Step 5/105 Loss 0.9607 StepTime 1.49s\n",
      "Epoch 1/3 Step 6/105 Loss 1.1455 StepTime 1.46s\n",
      "Epoch 1/3 Step 7/105 Loss 1.0501 StepTime 1.61s\n",
      "Epoch 1/3 Step 8/105 Loss 1.0739 StepTime 1.45s\n",
      "Epoch 1/3 Step 9/105 Loss 0.9084 StepTime 1.50s\n",
      "Epoch 1/3 Step 10/105 Loss 0.6019 StepTime 1.60s\n",
      "Epoch 1/3 Step 11/105 Loss 0.6468 StepTime 1.46s\n",
      "Epoch 1/3 Step 12/105 Loss 0.5016 StepTime 1.47s\n",
      "Epoch 1/3 Step 13/105 Loss 0.5530 StepTime 1.52s\n",
      "Epoch 1/3 Step 14/105 Loss 0.5949 StepTime 1.48s\n",
      "Epoch 1/3 Step 15/105 Loss 0.4602 StepTime 1.55s\n",
      "Epoch 1/3 Step 16/105 Loss 0.5601 StepTime 1.47s\n",
      "Epoch 1/3 Step 17/105 Loss 0.3620 StepTime 1.66s\n",
      "Epoch 1/3 Step 18/105 Loss 0.3713 StepTime 1.46s\n",
      "Epoch 1/3 Step 19/105 Loss 0.4288 StepTime 1.54s\n",
      "Epoch 1/3 Step 20/105 Loss 0.4281 StepTime 1.45s\n",
      "Epoch 1/3 Step 21/105 Loss 0.2082 StepTime 1.65s\n",
      "Epoch 1/3 Step 22/105 Loss 0.2253 StepTime 1.46s\n",
      "Epoch 1/3 Step 23/105 Loss 0.2844 StepTime 1.47s\n",
      "Epoch 1/3 Step 24/105 Loss 0.2314 StepTime 1.37s\n",
      "Epoch 1/3 Step 25/105 Loss 0.4836 StepTime 1.57s\n",
      "Epoch 1/3 Step 26/105 Loss 0.2777 StepTime 1.51s\n",
      "Epoch 1/3 Step 27/105 Loss 0.1300 StepTime 1.58s\n",
      "Epoch 1/3 Step 28/105 Loss 0.3326 StepTime 1.58s\n",
      "Epoch 1/3 Step 29/105 Loss 0.2203 StepTime 1.46s\n",
      "Epoch 1/3 Step 30/105 Loss 0.1206 StepTime 1.49s\n",
      "Epoch 1/3 Step 31/105 Loss 0.1655 StepTime 1.56s\n",
      "Epoch 1/3 Step 32/105 Loss 0.2722 StepTime 1.54s\n",
      "Epoch 1/3 Step 33/105 Loss 0.2049 StepTime 1.46s\n",
      "Epoch 1/3 Step 34/105 Loss 0.1032 StepTime 1.45s\n",
      "Epoch 1/3 Step 35/105 Loss 0.0865 StepTime 1.40s\n",
      "Epoch 2/3 Step 36/105 Loss 0.0779 StepTime 1.56s\n",
      "Epoch 2/3 Step 37/105 Loss 0.0522 StepTime 1.36s\n",
      "Epoch 2/3 Step 38/105 Loss 0.0868 StepTime 1.62s\n",
      "Epoch 2/3 Step 39/105 Loss 0.1499 StepTime 1.46s\n",
      "Epoch 2/3 Step 40/105 Loss 0.0904 StepTime 1.45s\n",
      "Epoch 2/3 Step 41/105 Loss 0.1007 StepTime 1.60s\n",
      "Epoch 2/3 Step 42/105 Loss 0.1701 StepTime 1.44s\n",
      "Epoch 2/3 Step 43/105 Loss 0.1354 StepTime 1.56s\n",
      "Epoch 2/3 Step 44/105 Loss 0.0817 StepTime 1.49s\n",
      "Epoch 2/3 Step 45/105 Loss 0.1574 StepTime 1.45s\n",
      "Epoch 2/3 Step 46/105 Loss 0.1537 StepTime 1.59s\n",
      "Epoch 2/3 Step 47/105 Loss 0.1023 StepTime 1.46s\n",
      "Epoch 2/3 Step 48/105 Loss 0.0531 StepTime 1.64s\n",
      "Epoch 2/3 Step 49/105 Loss 0.0831 StepTime 1.59s\n",
      "Epoch 2/3 Step 50/105 Loss 0.0877 StepTime 1.46s\n",
      "Epoch 2/3 Step 51/105 Loss 0.0486 StepTime 1.48s\n",
      "Epoch 2/3 Step 52/105 Loss 0.1040 StepTime 1.48s\n",
      "Epoch 2/3 Step 53/105 Loss 0.0686 StepTime 1.48s\n",
      "Epoch 2/3 Step 54/105 Loss 0.1799 StepTime 1.47s\n",
      "Epoch 2/3 Step 55/105 Loss 0.0769 StepTime 1.49s\n",
      "Epoch 2/3 Step 56/105 Loss 0.1519 StepTime 1.56s\n",
      "Epoch 2/3 Step 57/105 Loss 0.1240 StepTime 1.42s\n",
      "Epoch 2/3 Step 58/105 Loss 0.0838 StepTime 1.48s\n",
      "Epoch 2/3 Step 59/105 Loss 0.1196 StepTime 1.54s\n",
      "Epoch 2/3 Step 60/105 Loss 0.0740 StepTime 1.57s\n",
      "Epoch 2/3 Step 61/105 Loss 0.1175 StepTime 1.46s\n",
      "Epoch 2/3 Step 62/105 Loss 0.0506 StepTime 1.99s\n",
      "Epoch 2/3 Step 63/105 Loss 0.1424 StepTime 1.55s\n",
      "Epoch 2/3 Step 64/105 Loss 0.0555 StepTime 1.60s\n",
      "Epoch 2/3 Step 65/105 Loss 0.0338 StepTime 1.60s\n",
      "Epoch 2/3 Step 66/105 Loss 0.0723 StepTime 1.46s\n",
      "Epoch 2/3 Step 67/105 Loss 0.0452 StepTime 1.61s\n",
      "Epoch 2/3 Step 68/105 Loss 0.0865 StepTime 1.47s\n",
      "Epoch 2/3 Step 69/105 Loss 0.0998 StepTime 1.65s\n",
      "Epoch 2/3 Step 70/105 Loss 0.0943 StepTime 1.56s\n",
      "Epoch 3/3 Step 71/105 Loss 0.0799 StepTime 1.82s\n",
      "Epoch 3/3 Step 72/105 Loss 0.0351 StepTime 1.75s\n",
      "Epoch 3/3 Step 73/105 Loss 0.0252 StepTime 1.50s\n",
      "Epoch 3/3 Step 74/105 Loss 0.0298 StepTime 1.53s\n",
      "Epoch 3/3 Step 75/105 Loss 0.0250 StepTime 1.45s\n",
      "Epoch 3/3 Step 76/105 Loss 0.0222 StepTime 1.58s\n",
      "Epoch 3/3 Step 77/105 Loss 0.0217 StepTime 1.54s\n",
      "Epoch 3/3 Step 78/105 Loss 0.0142 StepTime 1.59s\n",
      "Epoch 3/3 Step 79/105 Loss 0.0251 StepTime 1.65s\n",
      "Epoch 3/3 Step 80/105 Loss 0.0192 StepTime 1.46s\n",
      "Epoch 3/3 Step 81/105 Loss 0.0325 StepTime 1.40s\n",
      "Epoch 3/3 Step 82/105 Loss 0.0281 StepTime 1.59s\n",
      "Epoch 3/3 Step 83/105 Loss 0.0279 StepTime 1.49s\n",
      "Epoch 3/3 Step 84/105 Loss 0.0132 StepTime 1.61s\n",
      "Epoch 3/3 Step 85/105 Loss 0.0214 StepTime 1.45s\n",
      "Epoch 3/3 Step 86/105 Loss 0.0539 StepTime 1.48s\n",
      "Epoch 3/3 Step 87/105 Loss 0.0238 StepTime 1.46s\n",
      "Epoch 3/3 Step 88/105 Loss 0.0125 StepTime 1.49s\n",
      "Epoch 3/3 Step 89/105 Loss 0.0309 StepTime 1.56s\n",
      "Epoch 3/3 Step 90/105 Loss 0.0171 StepTime 1.72s\n",
      "Epoch 3/3 Step 91/105 Loss 0.0079 StepTime 1.47s\n",
      "Epoch 3/3 Step 92/105 Loss 0.0249 StepTime 1.45s\n",
      "Epoch 3/3 Step 93/105 Loss 0.0320 StepTime 1.55s\n",
      "Epoch 3/3 Step 94/105 Loss 0.0168 StepTime 1.57s\n",
      "Epoch 3/3 Step 95/105 Loss 0.0232 StepTime 1.59s\n",
      "Epoch 3/3 Step 96/105 Loss 0.0396 StepTime 1.51s\n",
      "Epoch 3/3 Step 97/105 Loss 0.0165 StepTime 1.60s\n",
      "Epoch 3/3 Step 98/105 Loss 0.0114 StepTime 1.45s\n",
      "Epoch 3/3 Step 99/105 Loss 0.0273 StepTime 1.47s\n",
      "Epoch 3/3 Step 100/105 Loss 0.0320 StepTime 1.58s\n",
      "Epoch 3/3 Step 101/105 Loss 0.0418 StepTime 1.55s\n",
      "Epoch 3/3 Step 102/105 Loss 0.0937 StepTime 1.59s\n",
      "Epoch 3/3 Step 103/105 Loss 0.0165 StepTime 1.59s\n",
      "Epoch 3/3 Step 104/105 Loss 0.0108 StepTime 1.45s\n",
      "Epoch 3/3 Step 105/105 Loss 0.0178 StepTime 1.46s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "the vengeance of their enemies. Immediately they ran in crowds to the\n",
      "GOLD: e-thay engeance-vay of-ay eir-thay enemies-ay. ediately-Immay ey-thay an-ray in-ay owds-cray o-tay e-thay\n",
      "PRED: e-thay ecation-vay of-ay eir-thay enemies-ay. immediately-Im ed-thay an-ray in-ay ouds-cray o-tay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "gates were left open, are crowded together in them; then the Germans\n",
      "GOLD: ates-gay ere-way eft-lay open-ay, are-ay owded-cray ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "PRED: ates-gay ere-way eft-lay open-ay, are-ay ounded-cray ogether-tay in-ay em-thay; en-thay e-thay eman-Ger-ray\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "30 sentiments such of his fellow citizens as he has access to: he\n",
      "GOLD: 30 entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "PRED: 30 entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "X.--Roscius and Lucius Caesar, having received this message, went to\n",
      "GOLD: oscius-X.--Ray and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "PRED: o-Scius-RXay and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "border-men. Germans, G. i. 51\n",
      "GOLD: order-men-bay. ermans-Gay, G-ay. i-ay. 51\n",
      "PRED: order-bmen-bay. ermans-Gay, G-ay. i-ay. 51\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▃▂▃▂▃▄▃▂▅▃▄▄▂▁▅▄▄▂▅▃▃▄▂▅█▂▄▄▄▅▂▄▃▃▃▆▄▃▅▄</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅████████████</td></tr><tr><td>train/loss</td><td>█▄▄▃▃▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.24</td></tr><tr><td>perf/avg_step_time_s</td><td>1.52856</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>11191</td></tr><tr><td>perf/est_completion_tokens_total</td><td>33573</td></tr><tr><td>perf/step_time_s</td><td>1.46485</td></tr><tr><td>perf/wall_time_s</td><td>160.95002</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/loss</td><td>0.01784</td></tr><tr><td>train/step</td><td>105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-baseline-Qwen3-4B-Instruct-2507-20251212_125218</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/yed1fcme' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/yed1fcme</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_125218-yed1fcme\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=161.0 eval_exact_match_norm=0.240\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_name</th>\n",
       "      <th>base_model</th>\n",
       "      <th>preset</th>\n",
       "      <th>wall_time_s</th>\n",
       "      <th>avg_step_time_s</th>\n",
       "      <th>exact_match_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...</td>\n",
       "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>129.481849</td>\n",
       "      <td>2.693132</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>piglatin-baseline-Qwen3-4B-Instruct-2507-20251...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>baseline</td>\n",
       "      <td>160.950016</td>\n",
       "      <td>1.528557</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>piglatin-baseline-Llama-3.2-3B-20251212_124912</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>baseline</td>\n",
       "      <td>126.833512</td>\n",
       "      <td>1.203923</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>piglatin-light-Qwen3-4B-Instruct-2507-20251212...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>37.119052</td>\n",
       "      <td>1.542274</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>piglatin-light-Llama-3.2-3B-20251212_124034</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>light</td>\n",
       "      <td>28.620444</td>\n",
       "      <td>1.188445</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            run_name  \\\n",
       "2  piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...   \n",
       "4  piglatin-baseline-Qwen3-4B-Instruct-2507-20251...   \n",
       "3     piglatin-baseline-Llama-3.2-3B-20251212_124912   \n",
       "1  piglatin-light-Qwen3-4B-Instruct-2507-20251212...   \n",
       "0        piglatin-light-Llama-3.2-3B-20251212_124034   \n",
       "\n",
       "                         base_model    preset  wall_time_s  avg_step_time_s  \\\n",
       "2  Qwen/Qwen3-30B-A3B-Instruct-2507     light   129.481849         2.693132   \n",
       "4       Qwen/Qwen3-4B-Instruct-2507  baseline   160.950016         1.528557   \n",
       "3           meta-llama/Llama-3.2-3B  baseline   126.833512         1.203923   \n",
       "1       Qwen/Qwen3-4B-Instruct-2507     light    37.119052         1.542274   \n",
       "0           meta-llama/Llama-3.2-3B     light    28.620444         1.188445   \n",
       "\n",
       "   exact_match_norm  \n",
       "2              0.32  \n",
       "4              0.24  \n",
       "3              0.24  \n",
       "1              0.08  \n",
       "0              0.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RUN_TRAINING = True\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    USE_WANDB = True  # enable W&B for the real runs\n",
    "\n",
    "    results = []\n",
    "    for base_model, preset in EXPERIMENT_MATRIX:\n",
    "        print(\"\\n\\n### RUNNING:\", base_model, preset)\n",
    "        try:\n",
    "            out = run_one(base_model=base_model, preset_name=preset, seed=7, val_rows=25)\n",
    "            results.append(out)\n",
    "        except Exception as e:\n",
    "            print(\"!!! FAILED:\", base_model, preset, \"->\", repr(e))\n",
    "            results.append({\n",
    "                \"run_name\": None,\n",
    "                \"base_model\": base_model,\n",
    "                \"preset\": preset,\n",
    "                \"wall_time_s\": None,\n",
    "                \"avg_step_time_s\": None,\n",
    "                \"exact_match_norm\": None,\n",
    "                \"error\": repr(e),\n",
    "            })\n",
    "\n",
    "    df_local = pd.DataFrame(results)\n",
    "    display(df_local.sort_values(\"exact_match_norm\", ascending=False))\n",
    "else:\n",
    "    print(\"Skipping training (RUN_TRAINING=False).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2243fae-b0db-42e2-8c04-e0b66d4e0eb3",
   "metadata": {},
   "source": [
    "## 8) Post-run leaderboard (pull from W&B)\n",
    "\n",
    "Pull finished runs from W&B, filter to this notebook’s run naming convention, and build a leaderboard:\n",
    "- accuracy (eval/exact_match_norm)\n",
    "- wall time\n",
    "- step time\n",
    "- estimated completion tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e7fc6a8-b9f8-4c87-b7ef-7e3a19c418a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>base_model</th>\n",
       "      <th>preset</th>\n",
       "      <th>eval/exact_match_norm</th>\n",
       "      <th>perf/wall_time_s</th>\n",
       "      <th>perf/avg_step_time_s</th>\n",
       "      <th>perf/est_completion_tokens_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>piglatin-baseline-Qwen3-4B-Instruct-2507-20251...</td>\n",
       "      <td>50rlky0e</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.36</td>\n",
       "      <td>192.906789</td>\n",
       "      <td>1.832790</td>\n",
       "      <td>33573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...</td>\n",
       "      <td>qz48vlyt</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.32</td>\n",
       "      <td>108.945270</td>\n",
       "      <td>2.265083</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...</td>\n",
       "      <td>ka1ke3sz</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.32</td>\n",
       "      <td>129.481849</td>\n",
       "      <td>2.693132</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>piglatin-baseline-Llama-3.2-3B-20251212_124912</td>\n",
       "      <td>fvozjttu</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>126.833512</td>\n",
       "      <td>1.203923</td>\n",
       "      <td>33498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>piglatin-baseline-Qwen3-4B-Instruct-2507-20251...</td>\n",
       "      <td>yed1fcme</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>160.950016</td>\n",
       "      <td>1.528557</td>\n",
       "      <td>33573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>piglatin-baseline-Llama-3.2-3B-20251211_225651</td>\n",
       "      <td>gw99zgsf</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.20</td>\n",
       "      <td>124.723404</td>\n",
       "      <td>1.183409</td>\n",
       "      <td>33498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>piglatin-light-Qwen3-4B-Instruct-2507-20251212...</td>\n",
       "      <td>g76cpwcz</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.08</td>\n",
       "      <td>37.119052</td>\n",
       "      <td>1.542274</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>piglatin-light-Qwen3-4B-Instruct-2507-20251211...</td>\n",
       "      <td>ew0b2x8m</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.04</td>\n",
       "      <td>36.731605</td>\n",
       "      <td>1.526171</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>piglatin-light-Llama-3.2-3B-20251211_224809</td>\n",
       "      <td>dco1iwxq</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>light</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.274880</td>\n",
       "      <td>1.215609</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>piglatin-light-Llama-3.2-3B-20251212_124034</td>\n",
       "      <td>8byuow5e</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>light</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.620444</td>\n",
       "      <td>1.188445</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name        id  \\\n",
       "4  piglatin-baseline-Qwen3-4B-Instruct-2507-20251...  50rlky0e   \n",
       "2  piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...  qz48vlyt   \n",
       "7  piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...  ka1ke3sz   \n",
       "8     piglatin-baseline-Llama-3.2-3B-20251212_124912  fvozjttu   \n",
       "9  piglatin-baseline-Qwen3-4B-Instruct-2507-20251...  yed1fcme   \n",
       "3     piglatin-baseline-Llama-3.2-3B-20251211_225651  gw99zgsf   \n",
       "6  piglatin-light-Qwen3-4B-Instruct-2507-20251212...  g76cpwcz   \n",
       "1  piglatin-light-Qwen3-4B-Instruct-2507-20251211...  ew0b2x8m   \n",
       "0        piglatin-light-Llama-3.2-3B-20251211_224809  dco1iwxq   \n",
       "5        piglatin-light-Llama-3.2-3B-20251212_124034  8byuow5e   \n",
       "\n",
       "                                                 url  \\\n",
       "4  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "2  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "7  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "8  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "9  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "3  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "6  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "1  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "0  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "5  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "\n",
       "                         base_model    preset  eval/exact_match_norm  \\\n",
       "4       Qwen/Qwen3-4B-Instruct-2507  baseline                   0.36   \n",
       "2  Qwen/Qwen3-30B-A3B-Instruct-2507     light                   0.32   \n",
       "7  Qwen/Qwen3-30B-A3B-Instruct-2507     light                   0.32   \n",
       "8           meta-llama/Llama-3.2-3B  baseline                   0.24   \n",
       "9       Qwen/Qwen3-4B-Instruct-2507  baseline                   0.24   \n",
       "3           meta-llama/Llama-3.2-3B  baseline                   0.20   \n",
       "6       Qwen/Qwen3-4B-Instruct-2507     light                   0.08   \n",
       "1       Qwen/Qwen3-4B-Instruct-2507     light                   0.04   \n",
       "0           meta-llama/Llama-3.2-3B     light                   0.00   \n",
       "5           meta-llama/Llama-3.2-3B     light                   0.00   \n",
       "\n",
       "   perf/wall_time_s  perf/avg_step_time_s  perf/est_completion_tokens_total  \n",
       "4        192.906789              1.832790                             33573  \n",
       "2        108.945270              2.265083                              7724  \n",
       "7        129.481849              2.693132                              7724  \n",
       "8        126.833512              1.203923                             33498  \n",
       "9        160.950016              1.528557                             33573  \n",
       "3        124.723404              1.183409                             33498  \n",
       "6         37.119052              1.542274                              7724  \n",
       "1         36.731605              1.526171                              7724  \n",
       "0         29.274880              1.215609                              7710  \n",
       "5         28.620444              1.188445                              7710  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "ENTITY  = WANDB_ENTITY\n",
    "PROJECT = WANDB_PROJECT\n",
    "GROUP   = WANDB_GROUP\n",
    "\n",
    "api = wandb.Api()\n",
    "runs = api.runs(f\"{ENTITY}/{PROJECT}\", {\"state\": \"finished\"})\n",
    "\n",
    "rows = []\n",
    "for r in runs:\n",
    "    # filter to this notebook’s runs (and optionally group)\n",
    "    if not (r.name or \"\").startswith(\"piglatin-\"):\n",
    "        continue\n",
    "    if GROUP and (r.group != GROUP):\n",
    "        continue\n",
    "\n",
    "    s = dict(r.summary)\n",
    "    c = dict(r.config)\n",
    "\n",
    "    rows.append({\n",
    "        \"name\": r.name,\n",
    "        \"id\": r.id,\n",
    "        \"url\": r.url,\n",
    "        \"base_model\": c.get(\"base_model\") or c.get(\"model\") or c.get(\"model_name\"),\n",
    "        \"preset\": c.get(\"preset\"),\n",
    "        \"eval/exact_match_norm\": s.get(\"eval/exact_match_norm\"),\n",
    "        \"perf/wall_time_s\": s.get(\"perf/wall_time_s\"),\n",
    "        \"perf/avg_step_time_s\": s.get(\"perf/avg_step_time_s\"),\n",
    "        \"perf/est_completion_tokens_total\": s.get(\"perf/est_completion_tokens_total\"),\n",
    "    })\n",
    "\n",
    "df_wandb = pd.DataFrame(rows)\n",
    "\n",
    "df_wandb_sorted = df_wandb.sort_values(\"eval/exact_match_norm\", ascending=False)\n",
    "display(df_wandb_sorted.head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533316d-52d0-4c6e-8d9f-f220aa3bc731",
   "metadata": {},
   "source": [
    "## 9) One quick visualization (accuracy vs wall time)\n",
    "\n",
    "Simple scatter plot to see tradeoffs:\n",
    "- higher accuracy vs longer runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4aff8d3-f5d5-4091-8474-7aca5941a43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = df_wandb_sorted.dropna(subset=[\"perf/wall_time_s\", \"eval/exact_match_norm\"]).copy()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(plot_df[\"perf/wall_time_s\"], plot_df[\"eval/exact_match_norm\"])\n",
    "plt.xlabel(\"perf/wall_time_s\")\n",
    "plt.ylabel(\"eval/exact_match_norm\")\n",
    "plt.title(\"Pig Latin: accuracy vs wall time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e3ded-8964-4e43-9769-da1ba19341fc",
   "metadata": {},
   "source": [
    "## 7) Post-run leaderboard from W&B (latest per model + preset)\n",
    "\n",
    "W&B returns all finished runs in the project (including yesterday’s duplicates).\n",
    "For a clean “current results” view, we:\n",
    "\n",
    "- pull finished runs for this project/group\n",
    "- keep only runs that match our naming convention (`piglatin-...`)\n",
    "- **dedupe by keeping the most recent run per (`base_model`, `preset`)**\n",
    "- sort by `eval/exact_match_norm` (best accuracy first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b21646b7-4da4-449a-89fc-e7a6dd90fae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>base_model</th>\n",
       "      <th>preset</th>\n",
       "      <th>eval/exact_match_norm</th>\n",
       "      <th>perf/wall_time_s</th>\n",
       "      <th>perf/avg_step_time_s</th>\n",
       "      <th>perf/est_completion_tokens_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-12 17:43:50+00:00</td>\n",
       "      <td>piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...</td>\n",
       "      <td>ka1ke3sz</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.32</td>\n",
       "      <td>129.481849</td>\n",
       "      <td>2.693132</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-12 17:52:18+00:00</td>\n",
       "      <td>piglatin-baseline-Qwen3-4B-Instruct-2507-20251...</td>\n",
       "      <td>yed1fcme</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>160.950016</td>\n",
       "      <td>1.528557</td>\n",
       "      <td>33573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-12 17:49:12+00:00</td>\n",
       "      <td>piglatin-baseline-Llama-3.2-3B-20251212_124912</td>\n",
       "      <td>fvozjttu</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>126.833512</td>\n",
       "      <td>1.203923</td>\n",
       "      <td>33498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-12 17:42:08+00:00</td>\n",
       "      <td>piglatin-light-Qwen3-4B-Instruct-2507-20251212...</td>\n",
       "      <td>g76cpwcz</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.08</td>\n",
       "      <td>37.119052</td>\n",
       "      <td>1.542274</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-12 17:40:41+00:00</td>\n",
       "      <td>piglatin-light-Llama-3.2-3B-20251212_124034</td>\n",
       "      <td>8byuow5e</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>light</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.620444</td>\n",
       "      <td>1.188445</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at  \\\n",
       "0 2025-12-12 17:43:50+00:00   \n",
       "1 2025-12-12 17:52:18+00:00   \n",
       "2 2025-12-12 17:49:12+00:00   \n",
       "3 2025-12-12 17:42:08+00:00   \n",
       "4 2025-12-12 17:40:41+00:00   \n",
       "\n",
       "                                                name        id  \\\n",
       "0  piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...  ka1ke3sz   \n",
       "1  piglatin-baseline-Qwen3-4B-Instruct-2507-20251...  yed1fcme   \n",
       "2     piglatin-baseline-Llama-3.2-3B-20251212_124912  fvozjttu   \n",
       "3  piglatin-light-Qwen3-4B-Instruct-2507-20251212...  g76cpwcz   \n",
       "4        piglatin-light-Llama-3.2-3B-20251212_124034  8byuow5e   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "1  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "2  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "3  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "4  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "\n",
       "                         base_model    preset  eval/exact_match_norm  \\\n",
       "0  Qwen/Qwen3-30B-A3B-Instruct-2507     light                   0.32   \n",
       "1       Qwen/Qwen3-4B-Instruct-2507  baseline                   0.24   \n",
       "2           meta-llama/Llama-3.2-3B  baseline                   0.24   \n",
       "3       Qwen/Qwen3-4B-Instruct-2507     light                   0.08   \n",
       "4           meta-llama/Llama-3.2-3B     light                   0.00   \n",
       "\n",
       "   perf/wall_time_s  perf/avg_step_time_s  perf/est_completion_tokens_total  \n",
       "0        129.481849              2.693132                              7724  \n",
       "1        160.950016              1.528557                             33573  \n",
       "2        126.833512              1.203923                             33498  \n",
       "3         37.119052              1.542274                              7724  \n",
       "4         28.620444              1.188445                              7710  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 10 finished runs; showing 5 latest (1 per base_model+preset).\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "\n",
    "ENTITY = \"itprodirect\"\n",
    "PROJECT = \"tinker-hello-world\"\n",
    "GROUP = \"piglatin-multimodel-benchmark-v1\"\n",
    "NAME_PREFIX = \"piglatin-\"\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "runs = api.runs(\n",
    "    f\"{ENTITY}/{PROJECT}\",\n",
    "    filters={\"state\": \"finished\", \"group\": GROUP},\n",
    "    order=\"-created_at\",\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for r in runs:\n",
    "    name = r.name or \"\"\n",
    "    if not name.startswith(NAME_PREFIX):\n",
    "        continue\n",
    "\n",
    "    s = dict(r.summary) if r.summary is not None else {}\n",
    "    c = dict(r.config) if r.config is not None else {}\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"created_at\": r.created_at,\n",
    "            \"name\": name,\n",
    "            \"id\": r.id,\n",
    "            \"url\": r.url,\n",
    "            \"base_model\": c.get(\"base_model\") or c.get(\"model\") or c.get(\"model_name\"),\n",
    "            \"preset\": c.get(\"preset\"),\n",
    "            \"eval/exact_match_norm\": s.get(\"eval/exact_match_norm\"),\n",
    "            \"perf/wall_time_s\": s.get(\"perf/wall_time_s\"),\n",
    "            \"perf/avg_step_time_s\": s.get(\"perf/avg_step_time_s\"),\n",
    "            \"perf/est_completion_tokens_total\": s.get(\"perf/est_completion_tokens_total\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_wandb = pd.DataFrame(rows)\n",
    "df_wandb[\"created_at\"] = pd.to_datetime(df_wandb[\"created_at\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# Keep ONLY the latest run per (base_model, preset)\n",
    "df_wandb_latest = (\n",
    "    df_wandb.dropna(subset=[\"base_model\", \"preset\", \"created_at\"])\n",
    "    .sort_values(\"created_at\", ascending=False)\n",
    "    .groupby([\"base_model\", \"preset\"], as_index=False)\n",
    "    .head(1)\n",
    "    .sort_values(\"eval/exact_match_norm\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"created_at\",\n",
    "    \"name\",\n",
    "    \"id\",\n",
    "    \"url\",\n",
    "    \"base_model\",\n",
    "    \"preset\",\n",
    "    \"eval/exact_match_norm\",\n",
    "    \"perf/wall_time_s\",\n",
    "    \"perf/avg_step_time_s\",\n",
    "    \"perf/est_completion_tokens_total\",\n",
    "]\n",
    "\n",
    "display(df_wandb_latest[cols])\n",
    "print(f\"Pulled {len(df_wandb)} finished runs; showing {len(df_wandb_latest)} latest (1 per base_model+preset).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a50d6-4fb1-4780-94f7-5185421730a6",
   "metadata": {},
   "source": [
    "## 9) One quick visualization - Only 5 training runs from this notebook (accuracy vs wall time)\n",
    "\n",
    "Simple scatter plot to see tradeoffs:\n",
    "- higher accuracy vs longer runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa0e3ed5-fa67-4826-9987-4df77a3a581f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_df = df_wandb_latest.dropna(subset=[\"perf/wall_time_s\", \"eval/exact_match_norm\"]).copy()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(plot_df[\"perf/wall_time_s\"], plot_df[\"eval/exact_match_norm\"])\n",
    "plt.xlabel(\"perf/wall_time_s\")\n",
    "plt.ylabel(\"eval/exact_match_norm\")\n",
    "plt.title(\"Pig Latin: accuracy vs wall time (latest per config)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae77475-90da-482b-9282-c15bfd113ffe",
   "metadata": {},
   "source": [
    "## 8) Wrap-up — what we learned + what to do next\n",
    "\n",
    "### What we ran (this notebook)\n",
    "- Multi-model LoRA fine-tunes on a small Pig Latin dataset (`data/piglatin/sample.jsonl`)\n",
    "- Presets:\n",
    "  - **light**: quick + cheap sanity check\n",
    "  - **baseline**: longer runs for better performance\n",
    "- Models tested:\n",
    "  - meta-llama/Llama-3.2-3B\n",
    "  - Qwen/Qwen3-4B-Instruct-2507\n",
    "  - Qwen/Qwen3-30B-A3B-Instruct-2507 (with safer overrides)\n",
    "\n",
    "### Key results (latest per config)\n",
    "- Best **accuracy** from the latest runs: **Qwen3-30B (light)** (~0.32 exact_match_norm)\n",
    "- Best **speed/cheap sanity check**: **Llama-3.2-3B (light)** (~30s, low accuracy)\n",
    "- Best **balanced baseline**: **Qwen3-4B (baseline)** and **Llama-3.2-3B (baseline)** (~0.24 exact_match_norm)\n",
    "\n",
    "### Why duplicates happened (and how we fixed it)\n",
    "- W&B will return *all* historical runs unless we filter.\n",
    "- In analysis we prefer **latest run per (base_model, preset)** to avoid duplicates and keep comparisons clean.\n",
    "\n",
    "### Next experiments (high impact)\n",
    "- Increase training rows (e.g., 1k–5k) and hold val split constant\n",
    "- Try 1–2 hyperparam sweeps (LR, rank, epochs) on the best model family\n",
    "- Add a second metric (character-level edit distance) so near-misses count\n",
    "\n",
    "### W&B report (deliverable)\n",
    "We’ll export:\n",
    "- `results/piglatin_latest_per_config.csv` + `.json`\n",
    "- an optional W&B artifact containing the results snapshot\n",
    "Then create a W&B Report that includes:\n",
    "- Runs table (filtered to this group)\n",
    "- Scatter: accuracy vs wall time\n",
    "- A small “Top samples” panel showing predictions vs gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea495ea0-e12f-4b18-8283-dfbd316aa72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataframe: df_wandb_sorted  (rows=10)\n",
      "Latest per config: rows=5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_model</th>\n",
       "      <th>preset</th>\n",
       "      <th>eval/exact_match_norm</th>\n",
       "      <th>perf/wall_time_s</th>\n",
       "      <th>perf/avg_step_time_s</th>\n",
       "      <th>perf/est_completion_tokens_total</th>\n",
       "      <th>url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.32</td>\n",
       "      <td>108.945270</td>\n",
       "      <td>2.265083</td>\n",
       "      <td>7724</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>qz48vlyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.36</td>\n",
       "      <td>192.906789</td>\n",
       "      <td>1.832790</td>\n",
       "      <td>33573</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>50rlky0e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.08</td>\n",
       "      <td>37.119052</td>\n",
       "      <td>1.542274</td>\n",
       "      <td>7724</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>g76cpwcz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>126.833512</td>\n",
       "      <td>1.203923</td>\n",
       "      <td>33498</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>fvozjttu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>light</td>\n",
       "      <td>0.00</td>\n",
       "      <td>29.274880</td>\n",
       "      <td>1.215609</td>\n",
       "      <td>7710</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>dco1iwxq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         base_model    preset  eval/exact_match_norm  \\\n",
       "0  Qwen/Qwen3-30B-A3B-Instruct-2507     light                   0.32   \n",
       "1       Qwen/Qwen3-4B-Instruct-2507  baseline                   0.36   \n",
       "2       Qwen/Qwen3-4B-Instruct-2507     light                   0.08   \n",
       "3           meta-llama/Llama-3.2-3B  baseline                   0.24   \n",
       "4           meta-llama/Llama-3.2-3B     light                   0.00   \n",
       "\n",
       "   perf/wall_time_s  perf/avg_step_time_s  perf/est_completion_tokens_total  \\\n",
       "0        108.945270              2.265083                              7724   \n",
       "1        192.906789              1.832790                             33573   \n",
       "2         37.119052              1.542274                              7724   \n",
       "3        126.833512              1.203923                             33498   \n",
       "4         29.274880              1.215609                              7710   \n",
       "\n",
       "                                                 url        id  \n",
       "0  https://wandb.ai/itprodirect/tinker-hello-worl...  qz48vlyt  \n",
       "1  https://wandb.ai/itprodirect/tinker-hello-worl...  50rlky0e  \n",
       "2  https://wandb.ai/itprodirect/tinker-hello-worl...  g76cpwcz  \n",
       "3  https://wandb.ai/itprodirect/tinker-hello-worl...  fvozjttu  \n",
       "4  https://wandb.ai/itprodirect/tinker-hello-worl...  dco1iwxq  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved:\n",
      " - C:\\Users\\user\\Desktop\\tinker-hello-world\\results\\piglatin_latest_per_config_20251212_132110.csv\n",
      " - C:\\Users\\user\\Desktop\\tinker-hello-world\\results\\piglatin_latest_per_config_20251212_132110.json\n",
      " - C:\\Users\\user\\Desktop\\tinker-hello-world\\results\\requirements_freeze_20251212_132110.txt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_132111-7ms7vidu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/7ms7vidu' target=\"_blank\">piglatin-report-material-20251212_132110</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/7ms7vidu' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/7ms7vidu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-report-material-20251212_132110</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/7ms7vidu' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/7ms7vidu</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 6 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_132111-7ms7vidu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W&B artifact logged ✅ (snapshot + requirements freeze)\n",
      "\n",
      "Next step: Create a W&B Report (fast manual flow)\n",
      "1) Open your W&B project\n",
      "2) Filter runs by group = piglatin-multimodel-benchmark-v1\n",
      "3) Add panels: Runs Table + Scatter (accuracy vs wall_time) + Samples table\n",
      "4) Paste the report link back into the Wrap-up markdown cell\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Wrap-up export + W&B report helpers\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# ---- Where to save final outputs ----\n",
    "RESULTS_DIR = Path(REPO_ROOT) / \"results\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_path = RESULTS_DIR / f\"piglatin_latest_per_config_{ts}.csv\"\n",
    "json_path = RESULTS_DIR / f\"piglatin_latest_per_config_{ts}.json\"\n",
    "freeze_path = RESULTS_DIR / f\"requirements_freeze_{ts}.txt\"\n",
    "\n",
    "# ---- Pick the dataframe we already built in the notebook ----\n",
    "# (We try a few common names so this cell \"just works\".)\n",
    "df_candidates = {}\n",
    "for name in [\"df_wandb_sorted\", \"df_wandb\", \"df_runs\", \"df\"]:\n",
    "    if name in globals() and isinstance(globals()[name], pd.DataFrame):\n",
    "        df_candidates[name] = globals()[name]\n",
    "\n",
    "if not df_candidates:\n",
    "    raise RuntimeError(\n",
    "        \"No runs dataframe found. Expected one of: df_wandb_sorted, df_wandb, df_runs, df\"\n",
    "    )\n",
    "\n",
    "# Prefer df_wandb_sorted if present\n",
    "df_src_name = \"df_wandb_sorted\" if \"df_wandb_sorted\" in df_candidates else list(df_candidates.keys())[0]\n",
    "df_src = df_candidates[df_src_name].copy()\n",
    "\n",
    "required_cols = [\"base_model\", \"preset\", \"eval/exact_match_norm\", \"perf/wall_time_s\"]\n",
    "missing = [c for c in required_cols if c not in df_src.columns]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Runs dataframe '{df_src_name}' is missing columns: {missing}\")\n",
    "\n",
    "# ---- Keep only the latest run per (base_model, preset) ----\n",
    "# We use run_name timestamp suffix if available, else fall back to wall_time as a weak proxy.\n",
    "df_work = df_src.copy()\n",
    "\n",
    "def _extract_ts_from_run_name(run_name: str):\n",
    "    # expects suffix like 20251212_124034 in your naming\n",
    "    # returns sortable string, else empty\n",
    "    try:\n",
    "        parts = str(run_name).split(\"-\")\n",
    "        maybe = parts[-1]\n",
    "        # Accept either 20251212_124034 or similar\n",
    "        if len(maybe) >= 15 and \"_\" in maybe:\n",
    "            return maybe\n",
    "    except Exception:\n",
    "        pass\n",
    "    return \"\"\n",
    "\n",
    "if \"run_name\" in df_work.columns:\n",
    "    df_work[\"_run_ts\"] = df_work[\"run_name\"].map(_extract_ts_from_run_name)\n",
    "else:\n",
    "    df_work[\"_run_ts\"] = \"\"\n",
    "\n",
    "# Sort so \"latest\" is first, then drop duplicates by config\n",
    "df_latest = (\n",
    "    df_work.sort_values(by=[\"base_model\", \"preset\", \"_run_ts\"], ascending=[True, True, False])\n",
    "           .drop_duplicates(subset=[\"base_model\", \"preset\"], keep=\"first\")\n",
    "           .drop(columns=[\"_run_ts\"], errors=\"ignore\")\n",
    "           .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Helpful display order\n",
    "display_cols = []\n",
    "for c in [\"run_name\", \"base_model\", \"preset\", \"eval/exact_match_norm\", \"perf/wall_time_s\", \"perf/avg_step_time_s\", \"perf/est_completion_tokens_total\", \"url\", \"id\"]:\n",
    "    if c in df_latest.columns:\n",
    "        display_cols.append(c)\n",
    "\n",
    "print(f\"Using dataframe: {df_src_name}  (rows={len(df_src)})\")\n",
    "print(f\"Latest per config: rows={len(df_latest)}\")\n",
    "display(df_latest[display_cols] if display_cols else df_latest)\n",
    "\n",
    "# ---- Export snapshot ----\n",
    "df_latest.to_csv(csv_path, index=False)\n",
    "df_latest.to_json(json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", csv_path)\n",
    "print(\" -\", json_path)\n",
    "\n",
    "# ---- Freeze environment (repro) ----\n",
    "try:\n",
    "    out = subprocess.run([sys.executable, \"-m\", \"pip\", \"freeze\"], capture_output=True, text=True, check=False)\n",
    "    freeze_path.write_text(out.stdout, encoding=\"utf-8\")\n",
    "    print(\" -\", freeze_path)\n",
    "except Exception as e:\n",
    "    print(\"pip freeze failed (non-fatal):\", e)\n",
    "\n",
    "# ---- Optional: log snapshot as a W&B artifact + add a small summary table ----\n",
    "if (wandb is not None) and USE_WANDB:\n",
    "    try:\n",
    "        analysis_run = wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_ENTITY,\n",
    "            group=WANDB_GROUP,\n",
    "            job_type=\"analysis\",\n",
    "            name=f\"piglatin-report-material-{ts}\",\n",
    "            mode=WANDB_MODE,\n",
    "        )\n",
    "\n",
    "        # Log a compact table for quick report building\n",
    "        wandb.log({\"summary/latest_per_config\": wandb.Table(dataframe=df_latest[display_cols] if display_cols else df_latest)})\n",
    "\n",
    "        # Artifact with the exported files\n",
    "        art = wandb.Artifact(f\"piglatin_latest_per_config_{ts}\", type=\"benchmark_snapshot\")\n",
    "        art.add_file(str(csv_path))\n",
    "        art.add_file(str(json_path))\n",
    "        if freeze_path.exists():\n",
    "            art.add_file(str(freeze_path))\n",
    "\n",
    "        analysis_run.log_artifact(art)\n",
    "        wandb.finish()\n",
    "        print(\"\\nW&B artifact logged ✅ (snapshot + requirements freeze)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\nW&B logging failed (non-fatal):\", e)\n",
    "\n",
    "print(\"\\nNext step: Create a W&B Report (fast manual flow)\")\n",
    "print(\"1) Open your W&B project\")\n",
    "print(\"2) Filter runs by group =\", WANDB_GROUP)\n",
    "print(\"3) Add panels: Runs Table + Scatter (accuracy vs wall_time) + Samples table\")\n",
    "print(\"4) Paste the report link back into the Wrap-up markdown cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f91461-b753-421c-a98d-eff1fc1ad8c4",
   "metadata": {},
   "source": [
    "## Wrap-up: Pig Latin LoRA Benchmark v1 (multimodel)\n",
    "\n",
    "### Goal\n",
    "Run a small, repeatable LoRA benchmark across multiple base models and two presets (`light` vs `baseline`) and compare:\n",
    "- **Quality:** `eval/exact_match_norm`\n",
    "- **Speed:** `perf/wall_time_s` (and `perf/avg_step_time_s`)\n",
    "\n",
    "### What we ran\n",
    "- Dataset: `data/piglatin/sample.jsonl`\n",
    "- Presets:\n",
    "  - `light`: fast + cheap sanity check\n",
    "  - `baseline`: higher-capacity run (more rows/epochs/rank)\n",
    "- Tracked in W&B:\n",
    "  - Runs, configs, metrics\n",
    "  - “latest per config” aggregation (so duplicates don’t clutter analysis)\n",
    "\n",
    "### Key result (from this session)\n",
    "- Best point on the accuracy/speed tradeoff came from the latest `baseline` / `light` configs we selected.\n",
    "- **Important note:** exact match is strict — many outputs are “close but not exact”, so future iterations should add a softer metric (edit distance / char-F1 / token-F1) to better reflect real quality.\n",
    "\n",
    "### W&B Report (shareable write-up)\n",
    "- Report link: **PASTE_LINK_HERE**\n",
    "- Run set: 5 selected “latest” runs (multimodel comparison)\n",
    "\n",
    "### Next logical steps\n",
    "1) Add `base_model` + `preset` into `wandb.config` so W&B Reports can color/group by model cleanly.\n",
    "2) Add a “soft match” eval metric so results reflect near-misses.\n",
    "3) Expand the sweep space slightly (epochs, rank, rows) once metrics are solid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f05235e8-7ca2-4b11-afe5-8310fab3d05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataframe: df_wandb_latest  (rows=5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>base_model</th>\n",
       "      <th>preset</th>\n",
       "      <th>eval/exact_match_norm</th>\n",
       "      <th>perf/wall_time_s</th>\n",
       "      <th>perf/avg_step_time_s</th>\n",
       "      <th>perf/est_completion_tokens_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...</td>\n",
       "      <td>ka1ke3sz</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-30B-A3B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.32</td>\n",
       "      <td>129.481849</td>\n",
       "      <td>2.693132</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>piglatin-baseline-Qwen3-4B-Instruct-2507-20251...</td>\n",
       "      <td>yed1fcme</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>160.950016</td>\n",
       "      <td>1.528557</td>\n",
       "      <td>33573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>piglatin-baseline-Llama-3.2-3B-20251212_124912</td>\n",
       "      <td>fvozjttu</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.24</td>\n",
       "      <td>126.833512</td>\n",
       "      <td>1.203923</td>\n",
       "      <td>33498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>piglatin-light-Qwen3-4B-Instruct-2507-20251212...</td>\n",
       "      <td>g76cpwcz</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>Qwen/Qwen3-4B-Instruct-2507</td>\n",
       "      <td>light</td>\n",
       "      <td>0.08</td>\n",
       "      <td>37.119052</td>\n",
       "      <td>1.542274</td>\n",
       "      <td>7724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>piglatin-light-Llama-3.2-3B-20251212_124034</td>\n",
       "      <td>8byuow5e</td>\n",
       "      <td>https://wandb.ai/itprodirect/tinker-hello-worl...</td>\n",
       "      <td>meta-llama/Llama-3.2-3B</td>\n",
       "      <td>light</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.620444</td>\n",
       "      <td>1.188445</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name        id  \\\n",
       "0  piglatin-light-Qwen3-30B-A3B-Instruct-2507-202...  ka1ke3sz   \n",
       "1  piglatin-baseline-Qwen3-4B-Instruct-2507-20251...  yed1fcme   \n",
       "2     piglatin-baseline-Llama-3.2-3B-20251212_124912  fvozjttu   \n",
       "3  piglatin-light-Qwen3-4B-Instruct-2507-20251212...  g76cpwcz   \n",
       "4        piglatin-light-Llama-3.2-3B-20251212_124034  8byuow5e   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "1  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "2  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "3  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "4  https://wandb.ai/itprodirect/tinker-hello-worl...   \n",
       "\n",
       "                         base_model    preset  eval/exact_match_norm  \\\n",
       "0  Qwen/Qwen3-30B-A3B-Instruct-2507     light                   0.32   \n",
       "1       Qwen/Qwen3-4B-Instruct-2507  baseline                   0.24   \n",
       "2           meta-llama/Llama-3.2-3B  baseline                   0.24   \n",
       "3       Qwen/Qwen3-4B-Instruct-2507     light                   0.08   \n",
       "4           meta-llama/Llama-3.2-3B     light                   0.00   \n",
       "\n",
       "   perf/wall_time_s  perf/avg_step_time_s  perf/est_completion_tokens_total  \n",
       "0        129.481849              2.693132                              7724  \n",
       "1        160.950016              1.528557                             33573  \n",
       "2        126.833512              1.203923                             33498  \n",
       "3         37.119052              1.542274                              7724  \n",
       "4         28.620444              1.188445                              7710  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " - C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\results\\piglatin_latest_per_config_20251212_140022.csv\n",
      " - C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\results\\piglatin_latest_per_config_20251212_140022.json\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251212_140022-5zhvjo26</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/5zhvjo26' target=\"_blank\">piglatin-report-material-20251212_140022</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/5zhvjo26' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/5zhvjo26</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B: logged report material table + summary. (You can now paste the report URL into the wrap-up cell.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- pick the best available dataframe from your notebook ----\n",
    "df_for_report = None\n",
    "for _name in [\"df_latest_per_config\", \"df_wandb_latest\", \"df_wandb_sorted\", \"df_wandb\"]:\n",
    "    if _name in globals():\n",
    "        df_for_report = globals()[_name].copy()\n",
    "        print(f\"Using dataframe: {_name}  (rows={len(df_for_report)})\")\n",
    "        break\n",
    "\n",
    "if df_for_report is None:\n",
    "    raise ValueError(\"Couldn't find df_latest_per_config / df_wandb_sorted / df_wandb. Run the earlier W&B fetch/cleanup cells first.\")\n",
    "\n",
    "# ---- keep the most useful columns (only if they exist) ----\n",
    "preferred_cols = [\n",
    "    \"name\",\n",
    "    \"id\",\n",
    "    \"url\",\n",
    "    \"base_model\",\n",
    "    \"preset\",\n",
    "    \"eval/exact_match_norm\",\n",
    "    \"perf/wall_time_s\",\n",
    "    \"perf/avg_step_time_s\",\n",
    "    \"perf/est_completion_tokens_total\",\n",
    "    \"lora_rank\",\n",
    "    \"lora_alpha\",\n",
    "    \"learning_rate\",\n",
    "    \"batch_size\",\n",
    "    \"num_epochs\",\n",
    "    \"max_train_rows\",\n",
    "]\n",
    "cols = [c for c in preferred_cols if c in df_for_report.columns]\n",
    "df_key = df_for_report[cols].sort_values(by=\"eval/exact_match_norm\", ascending=False)\n",
    "\n",
    "display(df_key)\n",
    "\n",
    "# ---- save locally (timestamped) ----\n",
    "ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "csv_path = f\"results/piglatin_latest_per_config_{ts}.csv\"\n",
    "json_path = f\"results/piglatin_latest_per_config_{ts}.json\"\n",
    "\n",
    "df_key.to_csv(csv_path, index=False)\n",
    "df_key.to_json(json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", os.path.abspath(csv_path))\n",
    "print(\" -\", os.path.abspath(json_path))\n",
    "\n",
    "# ---- OPTIONAL: log a tiny “report material” run so the report has a stable artifact/table ----\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    if wandb.run is None:\n",
    "        wandb.init(\n",
    "            project=os.getenv(\"WANDB_PROJECT\", \"tinker-hello-world\"),\n",
    "            entity=os.getenv(\"WANDB_ENTITY\", \"itprodirect\"),\n",
    "            job_type=\"report_material\",\n",
    "            name=f\"piglatin-report-material-{ts}\",\n",
    "        )\n",
    "\n",
    "    wandb.log({\"report/summary_table\": wandb.Table(dataframe=df_key)})\n",
    "\n",
    "    best = df_key.iloc[0].to_dict()\n",
    "    wandb.run.summary[\"best_run_name\"] = best.get(\"name\")\n",
    "    wandb.run.summary[\"best_run_id\"] = best.get(\"id\")\n",
    "    if \"eval/exact_match_norm\" in best:\n",
    "        wandb.run.summary[\"best_exact_match_norm\"] = float(best[\"eval/exact_match_norm\"])\n",
    "    if \"perf/wall_time_s\" in best:\n",
    "        wandb.run.summary[\"best_wall_time_s\"] = float(best[\"perf/wall_time_s\"])\n",
    "\n",
    "    # paste your W&B report URL here once published:\n",
    "    wandb.run.summary[\"report_url\"] = \"PASTE_WANDB_REPORT_LINK_HERE\"\n",
    "\n",
    "    print(\"W&B: logged report material table + summary. (You can now paste the report URL into the wrap-up cell.)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Skipping W&B report-material logging:\", repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ed993-0e6f-4089-b518-ea7a6626c792",
   "metadata": {},
   "source": [
    "## Weights and Biases Report Link\n",
    "https://wandb.ai/itprodirect/tinker-hello-world/reports/Pig-Latin-LoRA-Benchmark-v1-multimodel---VmlldzoxNTM0ODU0MQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae438b-5ce0-4fd3-9b02-79d2370bd039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tinker-Hello-World (venv)",
   "language": "python",
   "name": "tinker-hello-world"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
