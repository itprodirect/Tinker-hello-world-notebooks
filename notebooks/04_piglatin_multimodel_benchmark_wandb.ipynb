{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f67ea6-93ad-415e-a104-d1bee4fc3653",
   "metadata": {},
   "source": [
    "# Notebook 04 — Pig Latin LoRA Multi-Model Benchmark (Tinker + W&B)\n",
    "\n",
    "**Goal:** Compare multiple base models on the same dataset and training setup, tracking:\n",
    "- training loss curves\n",
    "- wall time / step time\n",
    "- a simple quality metric (exact match on a small validation set)\n",
    "- a simple “cost proxy” (completion tokens per epoch)\n",
    "\n",
    "**Models tested:**\n",
    "- meta-llama/Llama-3.2-3B\n",
    "- Qwen/Qwen3-4B-Instruct-2507\n",
    "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
    "\n",
    "**Workflow:**\n",
    "1) Load environment + initialize Tinker client  \n",
    "2) Load dataset + sanity check  \n",
    "3) Define experiment matrix  \n",
    "4) Define helpers + training/eval function  \n",
    "5) Run a single smoke test  \n",
    "6) Run the full matrix + log to W&B (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e580d4e-c764-401b-8b37-cea3a5167296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import tinker\n",
    "from tinker import types\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734b6b0-c42b-43d9-8a59-edb336988d4f",
   "metadata": {},
   "source": [
    "## 1) Environment + Tinker client\n",
    "\n",
    "We explicitly load `.env` from the repo root (not the current notebook folder), then create a single `service_client`.\n",
    "This avoids “API key not found” and avoids re-creating sessions repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6399f5-dd4f-4bfe-87a8-2e793765c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: C:\\Users\\user\\Desktop\\tinker-hello-world\n",
      ".env exists: True\n",
      "TINKER_API_KEY present: True\n",
      "TINKER_API_KEY startswith: tml-CD\n",
      "ServiceClient ready ✅\n"
     ]
    }
   ],
   "source": [
    "def find_repo_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \".git\").exists() or (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "ENV_PATH = REPO_ROOT / \".env\"\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\".env exists:\", ENV_PATH.exists())\n",
    "\n",
    "load_dotenv(dotenv_path=ENV_PATH, override=True)\n",
    "\n",
    "api_key = os.getenv(\"TINKER_API_KEY\")\n",
    "print(\"TINKER_API_KEY present:\", bool(api_key))\n",
    "print(\"TINKER_API_KEY startswith:\", (api_key or \"\")[:6])\n",
    "\n",
    "assert api_key, \"Missing TINKER_API_KEY. Put it in repo-root .env as TINKER_API_KEY=...\"\n",
    "\n",
    "service_client = tinker.ServiceClient(api_key=api_key)\n",
    "print(\"ServiceClient ready ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3a163-658f-4441-a9d6-1a3c2e18c8bf",
   "metadata": {},
   "source": [
    "## 2) Dataset\n",
    "\n",
    "We load `data/piglatin/sample.jsonl` and verify it contains `{ \"input\": ..., \"output\": ... }` rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c44ca890-bfd4-4909-8b9b-c429340f4f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "Exists: True\n",
      "Rows: 300\n",
      "First row keys: dict_keys(['input', 'output'])\n",
      "First row sample: {'input': 'Translate this to Pig Latin:\\nHaving assembled the senate, he reminded them of the injustice of his', 'output': 'aving-Hay assembled-ay e-thay enate-say, e-hay eminded-ray em-thay of-ay e-thay injustice-ay of-ay is-hay'}\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = REPO_ROOT / \"data\" / \"piglatin\" / \"sample.jsonl\"\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n",
    "print(\"Exists:\", DATA_PATH.exists())\n",
    "assert DATA_PATH.exists(), f\"Missing dataset at {DATA_PATH}\"\n",
    "\n",
    "def read_jsonl(path):\n",
    "    path = Path(path)\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "rows = read_jsonl(DATA_PATH)\n",
    "print(\"Rows:\", len(rows))\n",
    "print(\"First row keys:\", rows[0].keys())\n",
    "print(\"First row sample:\", rows[0])\n",
    "\n",
    "assert \"input\" in rows[0] and \"output\" in rows[0], \"Dataset rows must contain 'input' and 'output'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba49a7-8f93-4b50-b7bd-4c4331338c20",
   "metadata": {},
   "source": [
    "## 3) Experiment matrix\n",
    "\n",
    "We define:\n",
    "- which base models to test\n",
    "- training presets (light vs baseline)\n",
    "- safe overrides for the 30B model\n",
    "- a run naming scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18508cc5-a9c5-42ae-b69b-178dc59b647c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planned runs:\n",
      " - meta-llama/Llama-3.2-3B light\n",
      " - Qwen/Qwen3-4B-Instruct-2507 light\n",
      " - Qwen/Qwen3-30B-A3B-Instruct-2507 light\n",
      " - meta-llama/Llama-3.2-3B baseline\n",
      " - Qwen/Qwen3-4B-Instruct-2507 baseline\n"
     ]
    }
   ],
   "source": [
    "WANDB_PROJECT = \"tinker-hello-world\"\n",
    "WANDB_ENTITY = None\n",
    "WANDB_MODE = \"online\"        # use \"offline\" if you want\n",
    "WANDB_GROUP = \"piglatin-multimodel-benchmark-v1\"\n",
    "USE_WANDB = False            # keep False for smoke test; turn True later\n",
    "\n",
    "BASE_MODELS = [\n",
    "    \"meta-llama/Llama-3.2-3B\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    "]\n",
    "\n",
    "PRESETS = {\n",
    "    \"light\": dict(max_train_rows=120, lora_rank=8,  lora_alpha=16, learning_rate=2e-4, num_epochs=2, batch_size=8),\n",
    "    \"baseline\": dict(max_train_rows=300, lora_rank=16, lora_alpha=32, learning_rate=1e-4, num_epochs=3, batch_size=8),\n",
    "}\n",
    "\n",
    "MODEL_OVERRIDES = {\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": dict(batch_size=4, max_train_rows=120),\n",
    "}\n",
    "\n",
    "EXPERIMENT_MATRIX = []\n",
    "for m in BASE_MODELS:\n",
    "    EXPERIMENT_MATRIX.append((m, \"light\"))\n",
    "for m in [\"meta-llama/Llama-3.2-3B\", \"Qwen/Qwen3-4B-Instruct-2507\"]:\n",
    "    EXPERIMENT_MATRIX.append((m, \"baseline\"))\n",
    "\n",
    "def make_run_name(base_model: str, preset: str) -> str:\n",
    "    short = base_model.split(\"/\")[-1]\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"piglatin-{preset}-{short}-{ts}\"\n",
    "\n",
    "print(\"Planned runs:\")\n",
    "for m, p in EXPERIMENT_MATRIX:\n",
    "    print(\" -\", m, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1817daa-741b-4ce3-88d5-e0c0c92e1254",
   "metadata": {},
   "source": [
    "## 4) Training example format\n",
    "\n",
    "We train next-token prediction where:\n",
    "- prompt tokens are weighted `0`\n",
    "- completion tokens are weighted `1`\n",
    "\n",
    "This makes the loss focus on the Pig Latin answer portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb122d0-4c5f-45ca-9209-48a2467c7a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    prompt = f\"English: {example['input']}\\nPig Latin:\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "\n",
    "    completion_tokens = tokenizer.encode(f\" {example['output']}\\n\\n\", add_special_tokens=False)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    "\n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "    weights = weights[1:]\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(weights=weights, target_tokens=target_tokens),\n",
    "    )\n",
    "\n",
    "def decode_completion(decoded: str) -> str:\n",
    "    if \"Pig Latin:\" in decoded:\n",
    "        decoded = decoded.split(\"Pig Latin:\")[-1]\n",
    "    return decoded.strip().splitlines()[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017d580-0ac4-4bb9-81ef-50de9304d04a",
   "metadata": {},
   "source": [
    "## 5) `run_one()` — train + evaluate one configuration\n",
    "\n",
    "Key design decisions:\n",
    "- `data_path=None` (prevents NameError during function definition)\n",
    "- cost proxy is computed from tokenizing the **outputs**, not from summing Tinker internals\n",
    "- W&B is optional and can be enabled after smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b8ad91-6fb1-4398-be34-df6fdf12373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Core training + eval cell\n",
    "# =========================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tinker\n",
    "from tinker import types\n",
    "\n",
    "# Optional: W&B\n",
    "try:\n",
    "    import wandb\n",
    "except Exception:\n",
    "    wandb = None\n",
    "\n",
    "# Optional: dotenv (loads TINKER_API_KEY if you use a .env file)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "except Exception:\n",
    "    load_dotenv = None\n",
    "\n",
    "\n",
    "# ---------- Paths / env ----------\n",
    "def find_repo_root(start: str | Path | None = None) -> Path:\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \".git\").exists() or (parent / \"pyproject.toml\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "ENV_PATH = REPO_ROOT / \".env\"\n",
    "\n",
    "if load_dotenv is not None and ENV_PATH.exists():\n",
    "    load_dotenv(dotenv_path=ENV_PATH, override=True)\n",
    "\n",
    "DATA_PATH = str(REPO_ROOT / \"data\" / \"piglatin\" / \"sample.jsonl\")\n",
    "\n",
    "\n",
    "# ---------- Config ----------\n",
    "WANDB_PROJECT = \"tinker-hello-world\"\n",
    "WANDB_ENTITY = None\n",
    "WANDB_MODE = \"online\"\n",
    "WANDB_GROUP = \"piglatin-multimodel-benchmark-v1\"\n",
    "USE_WANDB = True  # flip to False for smoke tests / debugging\n",
    "\n",
    "BASE_MODELS = [\n",
    "    \"meta-llama/Llama-3.2-3B\",\n",
    "    \"Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\",\n",
    "]\n",
    "\n",
    "PRESETS = {\n",
    "    \"light\": dict(max_train_rows=120, lora_rank=8,  lora_alpha=16, learning_rate=2e-4, num_epochs=2, batch_size=8),\n",
    "    \"baseline\": dict(max_train_rows=300, lora_rank=16, lora_alpha=32, learning_rate=1e-4, num_epochs=3, batch_size=8),\n",
    "}\n",
    "\n",
    "# Keep the 30B run safer on first pass\n",
    "MODEL_OVERRIDES = {\n",
    "    \"Qwen/Qwen3-30B-A3B-Instruct-2507\": dict(batch_size=4, max_train_rows=120),\n",
    "}\n",
    "\n",
    "# Run matrix: light for all, baseline for smaller ones\n",
    "EXPERIMENT_MATRIX = []\n",
    "for m in BASE_MODELS:\n",
    "    EXPERIMENT_MATRIX.append((m, \"light\"))\n",
    "for m in [\"meta-llama/Llama-3.2-3B\", \"Qwen/Qwen3-4B-Instruct-2507\"]:\n",
    "    EXPERIMENT_MATRIX.append((m, \"baseline\"))\n",
    "\n",
    "\n",
    "def make_run_name(base_model: str, preset: str) -> str:\n",
    "    short = base_model.split(\"/\")[-1]\n",
    "    ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return f\"piglatin-{preset}-{short}-{ts}\"\n",
    "\n",
    "\n",
    "# ---------- Data helpers ----------\n",
    "def read_jsonl(path: str | Path) -> list[dict]:\n",
    "    path = Path(path)\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "\n",
    "# ---------- Tinker helpers ----------\n",
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    \"\"\"\n",
    "    Build a next-token prediction datum:\n",
    "    - Prompt tokens are weight 0 (no loss)\n",
    "    - Completion tokens are weight 1 (loss)\n",
    "    \"\"\"\n",
    "    prompt = f\"English: {example['input']}\\nPig Latin:\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "\n",
    "    completion_tokens = tokenizer.encode(f\" {example['output']}\\n\", add_special_tokens=False)\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    "\n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "    weights = weights[1:]  # align with targets\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(weights=weights, target_tokens=target_tokens),\n",
    "    )\n",
    "\n",
    "\n",
    "def _as_1d(x) -> np.ndarray:\n",
    "    \"\"\"Force scalars / tuples / lists / arrays into a 1D numpy array.\"\"\"\n",
    "    if hasattr(x, \"tolist\"):\n",
    "        x = x.tolist()\n",
    "    arr = np.asarray(x)\n",
    "    return arr.reshape(-1)\n",
    "\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return \" \".join(s.strip().lower().split())\n",
    "\n",
    "\n",
    "# ---------- Main experiment ----------\n",
    "def run_one(\n",
    "    base_model: str,\n",
    "    preset_name: str,\n",
    "    data_path: str | None = None,\n",
    "    seed: int = 7,\n",
    "    val_rows: int = 25,\n",
    "):\n",
    "    if data_path is None:\n",
    "        data_path = DATA_PATH\n",
    "\n",
    "    # Client (reads TINKER_API_KEY from env)\n",
    "    service_client = tinker.ServiceClient()\n",
    "\n",
    "    cfg = dict(PRESETS[preset_name])\n",
    "    cfg.update(MODEL_OVERRIDES.get(base_model, {}))\n",
    "\n",
    "    run_name = make_run_name(base_model, preset_name)\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    examples = read_jsonl(data_path)[: cfg[\"max_train_rows\"]]\n",
    "    random.shuffle(examples)\n",
    "\n",
    "    val = examples[:val_rows]\n",
    "    train = examples[val_rows:]\n",
    "    assert len(train) > 0, \"Train split is empty; increase max_train_rows or reduce val_rows.\"\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"RUN:\", run_name)\n",
    "    print(\"MODEL:\", base_model)\n",
    "    print(\"CFG:\", cfg)\n",
    "    print(\"DATA:\", data_path)\n",
    "    print(\"==============================\")\n",
    "\n",
    "    wandb_run = None\n",
    "    if USE_WANDB and wandb is not None:\n",
    "        wandb_run = wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_ENTITY,\n",
    "            mode=WANDB_MODE,\n",
    "            name=run_name,\n",
    "            group=WANDB_GROUP,\n",
    "            tags=[preset_name, base_model.split(\"/\")[-1]],\n",
    "            config=dict(\n",
    "                base_model=base_model,\n",
    "                preset=preset_name,\n",
    "                **cfg,\n",
    "                seed=seed,\n",
    "                train_rows=len(train),\n",
    "                val_rows=len(val),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Training client + tokenizer\n",
    "    training_client = service_client.create_lora_training_client(\n",
    "        base_model=base_model,\n",
    "        rank=cfg[\"lora_rank\"],\n",
    "    )\n",
    "    tokenizer = training_client.get_tokenizer()\n",
    "\n",
    "    processed_train = [process_example(ex, tokenizer) for ex in train]\n",
    "\n",
    "    # Cost proxy (weighted tokens == completion tokens)\n",
    "    completion_tokens_per_epoch = int(\n",
    "        sum(_as_1d(d.loss_fn_inputs[\"weights\"]).sum() for d in processed_train)\n",
    "    )\n",
    "\n",
    "    num_examples = len(processed_train)\n",
    "    steps_per_epoch = int(np.ceil(num_examples / cfg[\"batch_size\"]))\n",
    "    total_steps = cfg[\"num_epochs\"] * steps_per_epoch\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    global_step = 0\n",
    "    step_times = []\n",
    "\n",
    "    for epoch in range(cfg[\"num_epochs\"]):\n",
    "        random.shuffle(processed_train)\n",
    "\n",
    "        for start in range(0, num_examples, cfg[\"batch_size\"]):\n",
    "            batch = processed_train[start : start + cfg[\"batch_size\"]]\n",
    "\n",
    "            st = time.perf_counter()\n",
    "\n",
    "            fwdbwd_future = training_client.forward_backward(batch, loss_fn=\"cross_entropy\")\n",
    "            optim_future = training_client.optim_step(types.AdamParams(learning_rate=cfg[\"learning_rate\"]))\n",
    "\n",
    "            fwdbwd_result = fwdbwd_future.result()\n",
    "            optim_future.result()\n",
    "\n",
    "            # Robust weighted token-level loss (same spirit as Notebook 03)\n",
    "            logprobs = np.concatenate([_as_1d(out[\"logprobs\"]) for out in fwdbwd_result.loss_fn_outputs])\n",
    "            weights = np.concatenate([_as_1d(ex.loss_fn_inputs[\"weights\"]) for ex in batch])\n",
    "\n",
    "            if logprobs.shape[0] != weights.shape[0]:\n",
    "                raise ValueError(f\"Length mismatch: logprobs={logprobs.shape} weights={weights.shape}\")\n",
    "\n",
    "            loss = -float(np.dot(logprobs, weights) / weights.sum())\n",
    "\n",
    "            global_step += 1\n",
    "            dt = time.perf_counter() - st\n",
    "            step_times.append(dt)\n",
    "\n",
    "            if USE_WANDB and wandb_run is not None:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss\": loss,\n",
    "                        \"train/epoch\": epoch + 1,\n",
    "                        \"train/step\": global_step,\n",
    "                        \"perf/step_time_s\": dt,\n",
    "                        \"perf/completion_tokens_per_epoch\": completion_tokens_per_epoch,\n",
    "                        \"perf/est_completion_tokens_total\": completion_tokens_per_epoch * cfg[\"num_epochs\"],\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{cfg['num_epochs']} Step {global_step}/{total_steps} Loss {loss:.4f} StepTime {dt:.2f}s\")\n",
    "\n",
    "    wall_time_s = time.perf_counter() - t0\n",
    "    avg_step_time_s = float(np.mean(step_times)) if step_times else None\n",
    "\n",
    "    # Save adapter + sampling client\n",
    "    sampling_client = training_client.save_weights_and_get_sampling_client(name=run_name)\n",
    "\n",
    "    # Eval (decode only completion tokens)\n",
    "    params = types.SamplingParams(max_tokens=60, temperature=0.0, top_p=1.0)\n",
    "\n",
    "    correct = 0\n",
    "    rows_out = []\n",
    "\n",
    "    for ex in val:\n",
    "        prompt = f\"English: {ex['input']}\\nPig Latin:\"\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "        model_input = types.ModelInput.from_ints(tokens=prompt_tokens)\n",
    "\n",
    "        result = sampling_client.sample(\n",
    "            prompt=model_input,\n",
    "            sampling_params=params,\n",
    "            num_samples=1\n",
    "        ).result()\n",
    "\n",
    "        seq_tokens = result.sequences[0].tokens\n",
    "\n",
    "        # Strip prompt prefix if present\n",
    "        if len(seq_tokens) >= len(prompt_tokens) and seq_tokens[:len(prompt_tokens)] == prompt_tokens:\n",
    "            new_tokens = seq_tokens[len(prompt_tokens):]\n",
    "        else:\n",
    "            new_tokens = seq_tokens\n",
    "\n",
    "        decoded_new = tokenizer.decode(new_tokens).strip()\n",
    "        pred = decoded_new.splitlines()[0].strip() if decoded_new else \"\"\n",
    "\n",
    "        gold = ex[\"output\"].strip()\n",
    "        is_ok = (norm(pred) == norm(gold))\n",
    "\n",
    "        correct += int(is_ok)\n",
    "        rows_out.append([ex[\"input\"], gold, pred, is_ok])\n",
    "\n",
    "        # Print first 5 samples for sanity\n",
    "        if len(rows_out) <= 5:\n",
    "            print(\"----\")\n",
    "            print(\"IN  :\", ex[\"input\"])\n",
    "            print(\"GOLD:\", gold)\n",
    "            print(\"PRED:\", pred if pred else \"<EMPTY>\")\n",
    "            print(\"OK? :\", is_ok)\n",
    "\n",
    "    exact_match = correct / len(val) if val else 0.0\n",
    "\n",
    "    if USE_WANDB and wandb_run is not None:\n",
    "        table = wandb.Table(columns=[\"input\", \"expected\", \"pred\", \"ok\"], data=rows_out[:10])\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"eval/exact_match_norm\": exact_match,\n",
    "                \"eval/samples\": table,\n",
    "                \"perf/wall_time_s\": wall_time_s,\n",
    "                \"perf/avg_step_time_s\": avg_step_time_s,\n",
    "            }\n",
    "        )\n",
    "        wandb_run.summary[\"perf/wall_time_s\"] = wall_time_s\n",
    "        wandb_run.summary[\"eval/exact_match_norm\"] = exact_match\n",
    "        wandb.finish()\n",
    "\n",
    "    print(f\"Done. wall_time_s={wall_time_s:.1f} eval_exact_match_norm={exact_match:.3f}\")\n",
    "    return {\n",
    "        \"run_name\": run_name,\n",
    "        \"wall_time_s\": wall_time_s,\n",
    "        \"avg_step_time_s\": avg_step_time_s,\n",
    "        \"exact_match_norm\": exact_match,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb08f8-d8ad-412c-8208-a43ad14f00ac",
   "metadata": {},
   "source": [
    "## 6) Smoke test\n",
    "\n",
    "Run exactly one small experiment (no W&B) to verify:\n",
    "- client works\n",
    "- dataset loads\n",
    "- training loop runs\n",
    "- evaluation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe04d53b-b45c-42c1-b3b1-f34b217c389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### RUNNING: meta-llama/Llama-3.2-3B light\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Llama-3.2-3B-20251211_224809\n",
      "MODEL: meta-llama/Llama-3.2-3B\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: nick99 (itprodirect) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251211_224813-dco1iwxq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/dco1iwxq' target=\"_blank\">piglatin-light-Llama-3.2-3B-20251211_224809</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/dco1iwxq' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/dco1iwxq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Step 1/24 Loss 3.2464 StepTime 1.34s\n",
      "Epoch 1/2 Step 2/24 Loss 2.9514 StepTime 1.21s\n",
      "Epoch 1/2 Step 3/24 Loss 2.3668 StepTime 1.22s\n",
      "Epoch 1/2 Step 4/24 Loss 1.9308 StepTime 1.17s\n",
      "Epoch 1/2 Step 5/24 Loss 1.7277 StepTime 1.18s\n",
      "Epoch 1/2 Step 6/24 Loss 1.6012 StepTime 1.27s\n",
      "Epoch 1/2 Step 7/24 Loss 1.3938 StepTime 1.26s\n",
      "Epoch 1/2 Step 8/24 Loss 1.0912 StepTime 1.18s\n",
      "Epoch 1/2 Step 9/24 Loss 1.0122 StepTime 1.20s\n",
      "Epoch 1/2 Step 10/24 Loss 0.8204 StepTime 1.24s\n",
      "Epoch 1/2 Step 11/24 Loss 0.5806 StepTime 1.19s\n",
      "Epoch 1/2 Step 12/24 Loss 0.5921 StepTime 1.19s\n",
      "Epoch 2/2 Step 13/24 Loss 0.4818 StepTime 1.16s\n",
      "Epoch 2/2 Step 14/24 Loss 0.3271 StepTime 1.26s\n",
      "Epoch 2/2 Step 15/24 Loss 0.3340 StepTime 1.27s\n",
      "Epoch 2/2 Step 16/24 Loss 0.3452 StepTime 1.12s\n",
      "Epoch 2/2 Step 17/24 Loss 0.2143 StepTime 1.25s\n",
      "Epoch 2/2 Step 18/24 Loss 0.3833 StepTime 1.24s\n",
      "Epoch 2/2 Step 19/24 Loss 0.3267 StepTime 1.30s\n",
      "Epoch 2/2 Step 20/24 Loss 0.4300 StepTime 1.19s\n",
      "Epoch 2/2 Step 21/24 Loss 0.3194 StepTime 1.11s\n",
      "Epoch 2/2 Step 22/24 Loss 0.2171 StepTime 1.19s\n",
      "Epoch 2/2 Step 23/24 Loss 0.2570 StepTime 1.18s\n",
      "Epoch 2/2 Step 24/24 Loss 0.1838 StepTime 1.25s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-Nay, an-ay ancient-ay ople-pay of-ay _Gallia-Gay ebelica-Bay_, outh-thay o-tay ave-hay etway in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: y-tray ever-whay ortune-fay aym-fay all-fay o-tay ere-lay ot-lay. If-aye ese-thay ings-aye ere-way o-taybe e-bay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-nay ot-nay e-bay ept-kay ack-bay y-bay e-thay irbuni-tay of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: usual-ay, o-tay otract-pray e-thay or-wayarsers-ay. ith-Whay ese-thay es-hay e-hay intermixed-ay a-ay uard-ay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oin-Viridovix, y-G. iii-ay. 17; o-tay alerci-Bannovices ordered-ay o-tay ursify-fay eir-thay\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>█▄▄▃▃▆▅▃▄▅▄▃▂▆▆▁▅▅▇▃▁▃▃▅</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>train/loss</td><td>█▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0</td></tr><tr><td>perf/avg_step_time_s</td><td>1.21561</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>3855</td></tr><tr><td>perf/est_completion_tokens_total</td><td>7710</td></tr><tr><td>perf/step_time_s</td><td>1.25344</td></tr><tr><td>perf/wall_time_s</td><td>29.27488</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>0.18381</td></tr><tr><td>train/step</td><td>24</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-light-Llama-3.2-3B-20251211_224809</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/dco1iwxq' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/dco1iwxq</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251211_224813-dco1iwxq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=29.3 eval_exact_match_norm=0.000\n",
      "\n",
      "\n",
      "### RUNNING: Qwen/Qwen3-4B-Instruct-2507 light\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Qwen3-4B-Instruct-2507-20251211_224952\n",
      "MODEL: Qwen/Qwen3-4B-Instruct-2507\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251211_224952-ew0b2x8m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/ew0b2x8m' target=\"_blank\">piglatin-light-Qwen3-4B-Instruct-2507-20251211_224952</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/ew0b2x8m' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/ew0b2x8m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Step 1/24 Loss 2.3160 StepTime 1.54s\n",
      "Epoch 1/2 Step 2/24 Loss 2.4552 StepTime 1.50s\n",
      "Epoch 1/2 Step 3/24 Loss 1.4947 StepTime 1.46s\n",
      "Epoch 1/2 Step 4/24 Loss 1.2732 StepTime 1.69s\n",
      "Epoch 1/2 Step 5/24 Loss 0.9144 StepTime 1.56s\n",
      "Epoch 1/2 Step 6/24 Loss 0.8122 StepTime 1.47s\n",
      "Epoch 1/2 Step 7/24 Loss 0.7046 StepTime 1.46s\n",
      "Epoch 1/2 Step 8/24 Loss 0.5047 StepTime 1.54s\n",
      "Epoch 1/2 Step 9/24 Loss 0.5939 StepTime 1.53s\n",
      "Epoch 1/2 Step 10/24 Loss 0.5199 StepTime 1.59s\n",
      "Epoch 1/2 Step 11/24 Loss 0.2109 StepTime 1.47s\n",
      "Epoch 1/2 Step 12/24 Loss 0.1673 StepTime 1.71s\n",
      "Epoch 2/2 Step 13/24 Loss 0.1878 StepTime 1.48s\n",
      "Epoch 2/2 Step 14/24 Loss 0.2043 StepTime 1.51s\n",
      "Epoch 2/2 Step 15/24 Loss 0.1281 StepTime 1.47s\n",
      "Epoch 2/2 Step 16/24 Loss 0.1277 StepTime 1.74s\n",
      "Epoch 2/2 Step 17/24 Loss 0.0882 StepTime 1.41s\n",
      "Epoch 2/2 Step 18/24 Loss 0.1461 StepTime 1.45s\n",
      "Epoch 2/2 Step 19/24 Loss 0.1831 StepTime 1.47s\n",
      "Epoch 2/2 Step 20/24 Loss 0.3136 StepTime 1.55s\n",
      "Epoch 2/2 Step 21/24 Loss 0.2143 StepTime 1.47s\n",
      "Epoch 2/2 Step 22/24 Loss 0.0896 StepTime 1.38s\n",
      "Epoch 2/2 Step 23/24 Loss 0.2066 StepTime 1.47s\n",
      "Epoch 2/2 Step 24/24 Loss 0.1458 StepTime 1.69s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-Nay, a-n aicent-ay eople-ay of-ay _ayllia-Belgica-Bay_, ought-thay o-tay ave-day elt-dway in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: y-ay atever-whay unture-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: ual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay, a-ay aturd-gay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oin-jay iridovix-Vay, G-ay. iii.-ay. 17-ay; ulerci-ay annovices-Bray ordered-ay o-stay insure-thay eir-thay\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▄▃▂▇▄▃▂▄▄▅▃▇▃▄▃█▂▂▃▄▃▁▃▇</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁████████████</td></tr><tr><td>train/loss</td><td>██▅▅▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.04</td></tr><tr><td>perf/avg_step_time_s</td><td>1.52617</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>3862</td></tr><tr><td>perf/est_completion_tokens_total</td><td>7724</td></tr><tr><td>perf/step_time_s</td><td>1.69472</td></tr><tr><td>perf/wall_time_s</td><td>36.7316</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>0.14582</td></tr><tr><td>train/step</td><td>24</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-light-Qwen3-4B-Instruct-2507-20251211_224952</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/ew0b2x8m' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/ew0b2x8m</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251211_224952-ew0b2x8m\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=36.7 eval_exact_match_norm=0.040\n",
      "\n",
      "\n",
      "### RUNNING: Qwen/Qwen3-30B-A3B-Instruct-2507 light\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-light-Qwen3-30B-A3B-Instruct-2507-20251211_225143\n",
      "MODEL: Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "CFG: {'max_train_rows': 120, 'lora_rank': 8, 'lora_alpha': 16, 'learning_rate': 0.0002, 'num_epochs': 2, 'batch_size': 4}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251211_225143-qz48vlyt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/qz48vlyt' target=\"_blank\">piglatin-light-Qwen3-30B-A3B-Instruct-2507-20251211_225143</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/qz48vlyt' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/qz48vlyt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abac87ea5e84425399a583573b743b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9523455120a4da4b2339e1be9e1f1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976ba48b605a43ec9c5f879f0db0fb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360b78c509264043a5c15495dcf618e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 Step 1/48 Loss 0.7406 StepTime 2.30s\n",
      "Epoch 1/2 Step 2/48 Loss 0.7771 StepTime 2.09s\n",
      "Epoch 1/2 Step 3/48 Loss 0.4513 StepTime 2.86s\n",
      "Epoch 1/2 Step 4/48 Loss 0.9059 StepTime 2.38s\n",
      "Epoch 1/2 Step 5/48 Loss 0.4594 StepTime 2.00s\n",
      "Epoch 1/2 Step 6/48 Loss 0.1700 StepTime 1.98s\n",
      "Epoch 1/2 Step 7/48 Loss 0.3442 StepTime 2.07s\n",
      "Epoch 1/2 Step 8/48 Loss 0.3202 StepTime 2.44s\n",
      "Epoch 1/2 Step 9/48 Loss 0.2918 StepTime 1.99s\n",
      "Epoch 1/2 Step 10/48 Loss 0.1661 StepTime 2.05s\n",
      "Epoch 1/2 Step 11/48 Loss 0.1953 StepTime 3.19s\n",
      "Epoch 1/2 Step 12/48 Loss 0.3437 StepTime 2.88s\n",
      "Epoch 1/2 Step 13/48 Loss 0.2982 StepTime 4.13s\n",
      "Epoch 1/2 Step 14/48 Loss 0.2668 StepTime 2.90s\n",
      "Epoch 1/2 Step 15/48 Loss 0.2236 StepTime 1.85s\n",
      "Epoch 1/2 Step 16/48 Loss 0.1160 StepTime 3.81s\n",
      "Epoch 1/2 Step 17/48 Loss 0.1929 StepTime 1.96s\n",
      "Epoch 1/2 Step 18/48 Loss 0.1847 StepTime 2.09s\n",
      "Epoch 1/2 Step 19/48 Loss 0.1256 StepTime 2.02s\n",
      "Epoch 1/2 Step 20/48 Loss 0.1304 StepTime 2.09s\n",
      "Epoch 1/2 Step 21/48 Loss 0.1068 StepTime 2.05s\n",
      "Epoch 1/2 Step 22/48 Loss 0.0560 StepTime 2.11s\n",
      "Epoch 1/2 Step 23/48 Loss 0.0465 StepTime 2.14s\n",
      "Epoch 1/2 Step 24/48 Loss 0.0276 StepTime 2.00s\n",
      "Epoch 2/2 Step 25/48 Loss 0.0210 StepTime 2.20s\n",
      "Epoch 2/2 Step 26/48 Loss 0.0328 StepTime 2.09s\n",
      "Epoch 2/2 Step 27/48 Loss 0.0580 StepTime 3.19s\n",
      "Epoch 2/2 Step 28/48 Loss 0.0461 StepTime 2.01s\n",
      "Epoch 2/2 Step 29/48 Loss 0.0818 StepTime 2.17s\n",
      "Epoch 2/2 Step 30/48 Loss 0.0180 StepTime 2.06s\n",
      "Epoch 2/2 Step 31/48 Loss 0.0250 StepTime 2.19s\n",
      "Epoch 2/2 Step 32/48 Loss 0.0814 StepTime 2.06s\n",
      "Epoch 2/2 Step 33/48 Loss 0.0914 StepTime 1.91s\n",
      "Epoch 2/2 Step 34/48 Loss 0.0099 StepTime 2.03s\n",
      "Epoch 2/2 Step 35/48 Loss 0.0406 StepTime 1.85s\n",
      "Epoch 2/2 Step 36/48 Loss 0.0459 StepTime 2.02s\n",
      "Epoch 2/2 Step 37/48 Loss 0.0351 StepTime 2.28s\n",
      "Epoch 2/2 Step 38/48 Loss 0.0681 StepTime 1.88s\n",
      "Epoch 2/2 Step 39/48 Loss 0.0514 StepTime 1.84s\n",
      "Epoch 2/2 Step 40/48 Loss 0.0629 StepTime 2.86s\n",
      "Epoch 2/2 Step 41/48 Loss 0.1214 StepTime 1.91s\n",
      "Epoch 2/2 Step 42/48 Loss 0.0901 StepTime 1.99s\n",
      "Epoch 2/2 Step 43/48 Loss 0.0156 StepTime 2.10s\n",
      "Epoch 2/2 Step 44/48 Loss 0.0297 StepTime 2.25s\n",
      "Epoch 2/2 Step 45/48 Loss 0.0765 StepTime 2.04s\n",
      "Epoch 2/2 Step 46/48 Loss 0.0380 StepTime 2.37s\n",
      "Epoch 2/2 Step 47/48 Loss 0.0294 StepTime 1.99s\n",
      "Epoch 2/2 Step 48/48 Loss 0.0204 StepTime 2.00s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "Nervii, an ancient people of _Gallia Belgica_, thought to have dwelt in\n",
      "GOLD: ervii-Nay, an-ay ancient-ay eople-pay of-ay allia-_Gay elgica_-Bay, ought-thay o-tay ave-hay elt-dway in-ay\n",
      "PRED: ervii-Nay, an-ay ancient-ay eople-pay of-ay _Gallia-ay elgica-_Bay, ought-thay o-tay ave-hay ewlt-day in-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "try whatever fortune may fall to their lot. If these things were to be\n",
      "GOLD: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "PRED: y-tray atever-whay ortune-fay ay-may all-fay o-tay eir-thay ot-lay. If-ay ese-thay ings-thay ere-way o-tay e-bay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "could not be kept back by the tribunes of the soldiers and the\n",
      "GOLD: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "PRED: ould-cay ot-nay e-bay ept-kay ack-bay y-bay e-thay ibunes-tray of-ay e-thay oldiers-say and-ay e-thay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "usual, to protect the foragers. With these he intermixed a guard of\n",
      "GOLD: usual-ay, o-tay otect-pray e-thay oragers-fay. ith-Way ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "PRED: esual-ay, o-tay otay otect-prey e-thay oragers-fay. ith-Whyay ese-thay e-hay intermixed-ay a-ay uard-gay of-ay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "join Viridovix, G. iii. 17; Aulerci Brannovices ordered to furnish their\n",
      "GOLD: oin-jay iridovix-Vay, G-ay. iii-ay. 17; ulerci-Aay annovices-Bray ordered-ay o-tay urnish-fay eir-thay\n",
      "PRED: oin-jay iridovix-Vay, ey-Gay. ii-ay. 17; awlerci-Aay annovices-Bray ordered-ay o-tay undish-fay eir-thay\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▂▂▄▃▁▂▃▁▂▅█▄▁▇▁▂▂▂▂▂▂▂▅▂▂▂▂▁▂▁▂▁▁▄▁▂▂▂▃▁</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>train/loss</td><td>▇▇▄█▅▄▃▃▂▂▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▁▂▂▁▁▁▁▁▁▂▁▁▂▁▁</td></tr><tr><td>train/step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.32</td></tr><tr><td>perf/avg_step_time_s</td><td>2.26508</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>3862</td></tr><tr><td>perf/est_completion_tokens_total</td><td>7724</td></tr><tr><td>perf/step_time_s</td><td>2.00046</td></tr><tr><td>perf/wall_time_s</td><td>108.94527</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/loss</td><td>0.02038</td></tr><tr><td>train/step</td><td>48</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-light-Qwen3-30B-A3B-Instruct-2507-20251211_225143</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/qz48vlyt' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/qz48vlyt</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251211_225143-qz48vlyt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=108.9 eval_exact_match_norm=0.320\n",
      "\n",
      "\n",
      "### RUNNING: meta-llama/Llama-3.2-3B baseline\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-baseline-Llama-3.2-3B-20251211_225651\n",
      "MODEL: meta-llama/Llama-3.2-3B\n",
      "CFG: {'max_train_rows': 300, 'lora_rank': 16, 'lora_alpha': 32, 'learning_rate': 0.0001, 'num_epochs': 3, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251211_225651-gw99zgsf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/gw99zgsf' target=\"_blank\">piglatin-baseline-Llama-3.2-3B-20251211_225651</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/gw99zgsf' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/gw99zgsf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Step 1/105 Loss 3.5147 StepTime 1.09s\n",
      "Epoch 1/3 Step 2/105 Loss 3.2103 StepTime 1.21s\n",
      "Epoch 1/3 Step 3/105 Loss 2.8060 StepTime 1.19s\n",
      "Epoch 1/3 Step 4/105 Loss 2.2765 StepTime 1.25s\n",
      "Epoch 1/3 Step 5/105 Loss 2.0698 StepTime 1.07s\n",
      "Epoch 1/3 Step 6/105 Loss 2.0122 StepTime 1.13s\n",
      "Epoch 1/3 Step 7/105 Loss 1.6839 StepTime 1.21s\n",
      "Epoch 1/3 Step 8/105 Loss 1.9408 StepTime 1.19s\n",
      "Epoch 1/3 Step 9/105 Loss 1.6102 StepTime 1.22s\n",
      "Epoch 1/3 Step 10/105 Loss 1.2944 StepTime 1.24s\n",
      "Epoch 1/3 Step 11/105 Loss 1.3189 StepTime 1.13s\n",
      "Epoch 1/3 Step 12/105 Loss 1.0346 StepTime 1.20s\n",
      "Epoch 1/3 Step 13/105 Loss 0.9026 StepTime 1.21s\n",
      "Epoch 1/3 Step 14/105 Loss 0.9712 StepTime 1.07s\n",
      "Epoch 1/3 Step 15/105 Loss 0.8658 StepTime 1.20s\n",
      "Epoch 1/3 Step 16/105 Loss 0.9484 StepTime 1.10s\n",
      "Epoch 1/3 Step 17/105 Loss 0.7535 StepTime 1.19s\n",
      "Epoch 1/3 Step 18/105 Loss 0.6817 StepTime 1.19s\n",
      "Epoch 1/3 Step 19/105 Loss 0.7975 StepTime 1.32s\n",
      "Epoch 1/3 Step 20/105 Loss 0.7419 StepTime 1.26s\n",
      "Epoch 1/3 Step 21/105 Loss 0.5979 StepTime 1.18s\n",
      "Epoch 1/3 Step 22/105 Loss 0.4172 StepTime 1.18s\n",
      "Epoch 1/3 Step 23/105 Loss 0.4356 StepTime 1.09s\n",
      "Epoch 1/3 Step 24/105 Loss 0.4041 StepTime 1.15s\n",
      "Epoch 1/3 Step 25/105 Loss 0.5784 StepTime 1.17s\n",
      "Epoch 1/3 Step 26/105 Loss 0.3856 StepTime 1.24s\n",
      "Epoch 1/3 Step 27/105 Loss 0.2846 StepTime 1.20s\n",
      "Epoch 1/3 Step 28/105 Loss 0.3295 StepTime 1.24s\n",
      "Epoch 1/3 Step 29/105 Loss 0.3506 StepTime 1.10s\n",
      "Epoch 1/3 Step 30/105 Loss 0.2440 StepTime 1.28s\n",
      "Epoch 1/3 Step 31/105 Loss 0.3128 StepTime 1.20s\n",
      "Epoch 1/3 Step 32/105 Loss 0.3588 StepTime 1.20s\n",
      "Epoch 1/3 Step 33/105 Loss 0.4099 StepTime 1.14s\n",
      "Epoch 1/3 Step 34/105 Loss 0.2190 StepTime 1.18s\n",
      "Epoch 1/3 Step 35/105 Loss 0.2176 StepTime 1.07s\n",
      "Epoch 2/3 Step 36/105 Loss 0.1512 StepTime 1.13s\n",
      "Epoch 2/3 Step 37/105 Loss 0.1235 StepTime 1.20s\n",
      "Epoch 2/3 Step 38/105 Loss 0.1601 StepTime 1.28s\n",
      "Epoch 2/3 Step 39/105 Loss 0.2174 StepTime 1.05s\n",
      "Epoch 2/3 Step 40/105 Loss 0.1591 StepTime 1.24s\n",
      "Epoch 2/3 Step 41/105 Loss 0.1140 StepTime 1.11s\n",
      "Epoch 2/3 Step 42/105 Loss 0.2598 StepTime 1.22s\n",
      "Epoch 2/3 Step 43/105 Loss 0.1751 StepTime 1.27s\n",
      "Epoch 2/3 Step 44/105 Loss 0.1568 StepTime 1.23s\n",
      "Epoch 2/3 Step 45/105 Loss 0.1625 StepTime 1.10s\n",
      "Epoch 2/3 Step 46/105 Loss 0.1692 StepTime 1.06s\n",
      "Epoch 2/3 Step 47/105 Loss 0.1300 StepTime 1.22s\n",
      "Epoch 2/3 Step 48/105 Loss 0.1145 StepTime 1.27s\n",
      "Epoch 2/3 Step 49/105 Loss 0.1103 StepTime 1.17s\n",
      "Epoch 2/3 Step 50/105 Loss 0.1457 StepTime 1.18s\n",
      "Epoch 2/3 Step 51/105 Loss 0.1192 StepTime 1.19s\n",
      "Epoch 2/3 Step 52/105 Loss 0.1668 StepTime 1.05s\n",
      "Epoch 2/3 Step 53/105 Loss 0.1267 StepTime 1.24s\n",
      "Epoch 2/3 Step 54/105 Loss 0.1865 StepTime 1.18s\n",
      "Epoch 2/3 Step 55/105 Loss 0.1277 StepTime 1.13s\n",
      "Epoch 2/3 Step 56/105 Loss 0.1958 StepTime 1.19s\n",
      "Epoch 2/3 Step 57/105 Loss 0.1574 StepTime 1.17s\n",
      "Epoch 2/3 Step 58/105 Loss 0.1176 StepTime 1.23s\n",
      "Epoch 2/3 Step 59/105 Loss 0.1255 StepTime 1.21s\n",
      "Epoch 2/3 Step 60/105 Loss 0.1434 StepTime 1.17s\n",
      "Epoch 2/3 Step 61/105 Loss 0.1436 StepTime 1.07s\n",
      "Epoch 2/3 Step 62/105 Loss 0.0986 StepTime 1.16s\n",
      "Epoch 2/3 Step 63/105 Loss 0.1753 StepTime 1.27s\n",
      "Epoch 2/3 Step 64/105 Loss 0.0870 StepTime 1.23s\n",
      "Epoch 2/3 Step 65/105 Loss 0.0655 StepTime 1.25s\n",
      "Epoch 2/3 Step 66/105 Loss 0.1176 StepTime 1.18s\n",
      "Epoch 2/3 Step 67/105 Loss 0.1035 StepTime 1.14s\n",
      "Epoch 2/3 Step 68/105 Loss 0.1414 StepTime 1.22s\n",
      "Epoch 2/3 Step 69/105 Loss 0.1352 StepTime 1.06s\n",
      "Epoch 2/3 Step 70/105 Loss 0.1543 StepTime 1.14s\n",
      "Epoch 3/3 Step 71/105 Loss 0.1225 StepTime 1.18s\n",
      "Epoch 3/3 Step 72/105 Loss 0.0472 StepTime 1.25s\n",
      "Epoch 3/3 Step 73/105 Loss 0.0435 StepTime 1.08s\n",
      "Epoch 3/3 Step 74/105 Loss 0.0541 StepTime 1.21s\n",
      "Epoch 3/3 Step 75/105 Loss 0.0289 StepTime 1.13s\n",
      "Epoch 3/3 Step 76/105 Loss 0.0364 StepTime 1.05s\n",
      "Epoch 3/3 Step 77/105 Loss 0.0414 StepTime 1.29s\n",
      "Epoch 3/3 Step 78/105 Loss 0.0278 StepTime 1.17s\n",
      "Epoch 3/3 Step 79/105 Loss 0.0601 StepTime 1.18s\n",
      "Epoch 3/3 Step 80/105 Loss 0.0273 StepTime 1.18s\n",
      "Epoch 3/3 Step 81/105 Loss 0.0653 StepTime 1.26s\n",
      "Epoch 3/3 Step 82/105 Loss 0.0819 StepTime 1.22s\n",
      "Epoch 3/3 Step 83/105 Loss 0.0378 StepTime 1.20s\n",
      "Epoch 3/3 Step 84/105 Loss 0.0405 StepTime 1.07s\n",
      "Epoch 3/3 Step 85/105 Loss 0.0545 StepTime 1.06s\n",
      "Epoch 3/3 Step 86/105 Loss 0.0767 StepTime 1.18s\n",
      "Epoch 3/3 Step 87/105 Loss 0.0369 StepTime 1.18s\n",
      "Epoch 3/3 Step 88/105 Loss 0.0588 StepTime 1.18s\n",
      "Epoch 3/3 Step 89/105 Loss 0.0374 StepTime 1.05s\n",
      "Epoch 3/3 Step 90/105 Loss 0.0336 StepTime 1.18s\n",
      "Epoch 3/3 Step 91/105 Loss 0.0415 StepTime 1.20s\n",
      "Epoch 3/3 Step 92/105 Loss 0.0623 StepTime 1.26s\n",
      "Epoch 3/3 Step 93/105 Loss 0.0337 StepTime 1.12s\n",
      "Epoch 3/3 Step 94/105 Loss 0.0406 StepTime 1.65s\n",
      "Epoch 3/3 Step 95/105 Loss 0.0581 StepTime 1.06s\n",
      "Epoch 3/3 Step 96/105 Loss 0.0548 StepTime 1.27s\n",
      "Epoch 3/3 Step 97/105 Loss 0.0422 StepTime 1.14s\n",
      "Epoch 3/3 Step 98/105 Loss 0.0436 StepTime 1.29s\n",
      "Epoch 3/3 Step 99/105 Loss 0.0399 StepTime 1.16s\n",
      "Epoch 3/3 Step 100/105 Loss 0.0579 StepTime 1.11s\n",
      "Epoch 3/3 Step 101/105 Loss 0.0752 StepTime 1.38s\n",
      "Epoch 3/3 Step 102/105 Loss 0.0939 StepTime 1.14s\n",
      "Epoch 3/3 Step 103/105 Loss 0.0565 StepTime 1.20s\n",
      "Epoch 3/3 Step 104/105 Loss 0.0326 StepTime 1.18s\n",
      "Epoch 3/3 Step 105/105 Loss 0.0269 StepTime 1.24s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "the vengeance of their enemies. Immediately they ran in crowds to the\n",
      "GOLD: e-thay engeance-vay of-ay eir-thay enemies-ay. ediately-Immay ey-thay an-ray in-ay owds-cray o-tay e-thay\n",
      "PRED: e-thay engeance-vay of-ay eir-thay enemies-ay. imediately-ay ey-thay an-ray in-ay owds-cay o-tay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "gates were left open, are crowded together in them; then the Germans\n",
      "GOLD: ates-gay ere-way eft-lay open-ay, are-ay owded-cray ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "PRED: ates-gay ere-way eft-lay open-ay, are-ay owdroc-cay ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "30 sentiments such of his fellow citizens as he has access to: he\n",
      "GOLD: 30 entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "PRED: 30-ay entiments-say uch-say of-ay is-hay elow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "X.--Roscius and Lucius Caesar, having received this message, went to\n",
      "GOLD: oscius-X.--Ray and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "PRED: uix.--Roscius-Xay and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "border-men. Germans, G. i. 51\n",
      "GOLD: order-men-bay. ermans-Gay, G-ay. i-ay. 51\n",
      "PRED: order-bay-men. erns-Gay, G-ay. i-ay. 51\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▅▃▅▅▆▂▄█▆▄▅▆▂▇▃▅▇▁▆▃▇▁▄▁▆▃▆▄▇▅▅▅▂▄▄▆▃▁▂▃</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train/loss</td><td>█▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.2</td></tr><tr><td>perf/avg_step_time_s</td><td>1.18341</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>11166</td></tr><tr><td>perf/est_completion_tokens_total</td><td>33498</td></tr><tr><td>perf/step_time_s</td><td>1.24072</td></tr><tr><td>perf/wall_time_s</td><td>124.7234</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/loss</td><td>0.0269</td></tr><tr><td>train/step</td><td>105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-baseline-Llama-3.2-3B-20251211_225651</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/gw99zgsf' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/gw99zgsf</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251211_225651-gw99zgsf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=124.7 eval_exact_match_norm=0.200\n",
      "\n",
      "\n",
      "### RUNNING: Qwen/Qwen3-4B-Instruct-2507 baseline\n",
      "\n",
      "==============================\n",
      "RUN: piglatin-baseline-Qwen3-4B-Instruct-2507-20251211_230009\n",
      "MODEL: Qwen/Qwen3-4B-Instruct-2507\n",
      "CFG: {'max_train_rows': 300, 'lora_rank': 16, 'lora_alpha': 32, 'learning_rate': 0.0001, 'num_epochs': 3, 'batch_size': 8}\n",
      "DATA: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251211_230009-50rlky0e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/50rlky0e' target=\"_blank\">piglatin-baseline-Qwen3-4B-Instruct-2507-20251211_230009</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/50rlky0e' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/50rlky0e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 Step 1/105 Loss 3.1657 StepTime 1.53s\n",
      "Epoch 1/3 Step 2/105 Loss 2.2812 StepTime 1.48s\n",
      "Epoch 1/3 Step 3/105 Loss 1.8943 StepTime 1.56s\n",
      "Epoch 1/3 Step 4/105 Loss 1.4823 StepTime 1.78s\n",
      "Epoch 1/3 Step 5/105 Loss 0.9688 StepTime 1.50s\n",
      "Epoch 1/3 Step 6/105 Loss 1.1643 StepTime 1.56s\n",
      "Epoch 1/3 Step 7/105 Loss 1.0727 StepTime 1.47s\n",
      "Epoch 1/3 Step 8/105 Loss 1.0806 StepTime 1.61s\n",
      "Epoch 1/3 Step 9/105 Loss 0.9097 StepTime 1.49s\n",
      "Epoch 1/3 Step 10/105 Loss 0.6108 StepTime 1.52s\n",
      "Epoch 1/3 Step 11/105 Loss 0.6487 StepTime 5.90s\n",
      "Epoch 1/3 Step 12/105 Loss 0.4989 StepTime 7.38s\n",
      "Epoch 1/3 Step 13/105 Loss 0.5551 StepTime 9.81s\n",
      "Epoch 1/3 Step 14/105 Loss 0.5976 StepTime 14.98s\n",
      "Epoch 1/3 Step 15/105 Loss 0.4659 StepTime 1.45s\n",
      "Epoch 1/3 Step 16/105 Loss 0.5718 StepTime 1.47s\n",
      "Epoch 1/3 Step 17/105 Loss 0.3572 StepTime 1.55s\n",
      "Epoch 1/3 Step 18/105 Loss 0.3796 StepTime 1.70s\n",
      "Epoch 1/3 Step 19/105 Loss 0.4370 StepTime 1.48s\n",
      "Epoch 1/3 Step 20/105 Loss 0.4340 StepTime 1.40s\n",
      "Epoch 1/3 Step 21/105 Loss 0.2020 StepTime 1.58s\n",
      "Epoch 1/3 Step 22/105 Loss 0.2293 StepTime 1.47s\n",
      "Epoch 1/3 Step 23/105 Loss 0.2856 StepTime 1.48s\n",
      "Epoch 1/3 Step 24/105 Loss 0.2282 StepTime 1.44s\n",
      "Epoch 1/3 Step 25/105 Loss 0.4935 StepTime 1.58s\n",
      "Epoch 1/3 Step 26/105 Loss 0.2780 StepTime 1.50s\n",
      "Epoch 1/3 Step 27/105 Loss 0.1299 StepTime 1.53s\n",
      "Epoch 1/3 Step 28/105 Loss 0.3428 StepTime 1.51s\n",
      "Epoch 1/3 Step 29/105 Loss 0.2182 StepTime 1.48s\n",
      "Epoch 1/3 Step 30/105 Loss 0.1205 StepTime 1.53s\n",
      "Epoch 1/3 Step 31/105 Loss 0.1642 StepTime 1.45s\n",
      "Epoch 1/3 Step 32/105 Loss 0.2746 StepTime 1.45s\n",
      "Epoch 1/3 Step 33/105 Loss 0.2095 StepTime 1.56s\n",
      "Epoch 1/3 Step 34/105 Loss 0.1013 StepTime 1.43s\n",
      "Epoch 1/3 Step 35/105 Loss 0.0853 StepTime 1.54s\n",
      "Epoch 2/3 Step 36/105 Loss 0.0781 StepTime 1.47s\n",
      "Epoch 2/3 Step 37/105 Loss 0.0522 StepTime 1.62s\n",
      "Epoch 2/3 Step 38/105 Loss 0.0845 StepTime 1.48s\n",
      "Epoch 2/3 Step 39/105 Loss 0.1553 StepTime 1.56s\n",
      "Epoch 2/3 Step 40/105 Loss 0.0899 StepTime 1.47s\n",
      "Epoch 2/3 Step 41/105 Loss 0.1019 StepTime 1.54s\n",
      "Epoch 2/3 Step 42/105 Loss 0.1720 StepTime 1.48s\n",
      "Epoch 2/3 Step 43/105 Loss 0.1379 StepTime 1.46s\n",
      "Epoch 2/3 Step 44/105 Loss 0.0778 StepTime 1.46s\n",
      "Epoch 2/3 Step 45/105 Loss 0.1639 StepTime 1.48s\n",
      "Epoch 2/3 Step 46/105 Loss 0.1469 StepTime 1.49s\n",
      "Epoch 2/3 Step 47/105 Loss 0.1011 StepTime 1.53s\n",
      "Epoch 2/3 Step 48/105 Loss 0.0524 StepTime 1.61s\n",
      "Epoch 2/3 Step 49/105 Loss 0.0840 StepTime 2.32s\n",
      "Epoch 2/3 Step 50/105 Loss 0.0921 StepTime 1.46s\n",
      "Epoch 2/3 Step 51/105 Loss 0.0469 StepTime 1.42s\n",
      "Epoch 2/3 Step 52/105 Loss 0.1059 StepTime 1.47s\n",
      "Epoch 2/3 Step 53/105 Loss 0.0756 StepTime 1.49s\n",
      "Epoch 2/3 Step 54/105 Loss 0.1778 StepTime 1.59s\n",
      "Epoch 2/3 Step 55/105 Loss 0.0760 StepTime 1.44s\n",
      "Epoch 2/3 Step 56/105 Loss 0.1573 StepTime 1.52s\n",
      "Epoch 2/3 Step 57/105 Loss 0.1044 StepTime 1.48s\n",
      "Epoch 2/3 Step 58/105 Loss 0.0848 StepTime 1.50s\n",
      "Epoch 2/3 Step 59/105 Loss 0.1083 StepTime 1.58s\n",
      "Epoch 2/3 Step 60/105 Loss 0.0753 StepTime 1.49s\n",
      "Epoch 2/3 Step 61/105 Loss 0.1103 StepTime 1.35s\n",
      "Epoch 2/3 Step 62/105 Loss 0.0369 StepTime 1.45s\n",
      "Epoch 2/3 Step 63/105 Loss 0.1483 StepTime 1.48s\n",
      "Epoch 2/3 Step 64/105 Loss 0.0533 StepTime 1.66s\n",
      "Epoch 2/3 Step 65/105 Loss 0.0340 StepTime 1.55s\n",
      "Epoch 2/3 Step 66/105 Loss 0.0692 StepTime 1.69s\n",
      "Epoch 2/3 Step 67/105 Loss 0.0468 StepTime 1.60s\n",
      "Epoch 2/3 Step 68/105 Loss 0.0842 StepTime 1.48s\n",
      "Epoch 2/3 Step 69/105 Loss 0.0992 StepTime 1.45s\n",
      "Epoch 2/3 Step 70/105 Loss 0.0963 StepTime 1.44s\n",
      "Epoch 3/3 Step 71/105 Loss 0.0869 StepTime 1.61s\n",
      "Epoch 3/3 Step 72/105 Loss 0.0326 StepTime 1.51s\n",
      "Epoch 3/3 Step 73/105 Loss 0.0271 StepTime 1.48s\n",
      "Epoch 3/3 Step 74/105 Loss 0.0343 StepTime 1.59s\n",
      "Epoch 3/3 Step 75/105 Loss 0.0208 StepTime 1.56s\n",
      "Epoch 3/3 Step 76/105 Loss 0.0247 StepTime 1.56s\n",
      "Epoch 3/3 Step 77/105 Loss 0.0213 StepTime 1.53s\n",
      "Epoch 3/3 Step 78/105 Loss 0.0153 StepTime 1.66s\n",
      "Epoch 3/3 Step 79/105 Loss 0.0254 StepTime 1.48s\n",
      "Epoch 3/3 Step 80/105 Loss 0.0203 StepTime 1.44s\n",
      "Epoch 3/3 Step 81/105 Loss 0.0269 StepTime 1.54s\n",
      "Epoch 3/3 Step 82/105 Loss 0.0301 StepTime 1.41s\n",
      "Epoch 3/3 Step 83/105 Loss 0.0282 StepTime 1.72s\n",
      "Epoch 3/3 Step 84/105 Loss 0.0134 StepTime 1.66s\n",
      "Epoch 3/3 Step 85/105 Loss 0.0257 StepTime 1.48s\n",
      "Epoch 3/3 Step 86/105 Loss 0.0492 StepTime 1.48s\n",
      "Epoch 3/3 Step 87/105 Loss 0.0266 StepTime 1.74s\n",
      "Epoch 3/3 Step 88/105 Loss 0.0135 StepTime 1.52s\n",
      "Epoch 3/3 Step 89/105 Loss 0.0344 StepTime 1.45s\n",
      "Epoch 3/3 Step 90/105 Loss 0.0156 StepTime 1.52s\n",
      "Epoch 3/3 Step 91/105 Loss 0.0073 StepTime 1.52s\n",
      "Epoch 3/3 Step 92/105 Loss 0.0280 StepTime 1.70s\n",
      "Epoch 3/3 Step 93/105 Loss 0.0364 StepTime 1.47s\n",
      "Epoch 3/3 Step 94/105 Loss 0.0171 StepTime 1.53s\n",
      "Epoch 3/3 Step 95/105 Loss 0.0242 StepTime 1.48s\n",
      "Epoch 3/3 Step 96/105 Loss 0.0347 StepTime 1.46s\n",
      "Epoch 3/3 Step 97/105 Loss 0.0188 StepTime 1.46s\n",
      "Epoch 3/3 Step 98/105 Loss 0.0120 StepTime 1.67s\n",
      "Epoch 3/3 Step 99/105 Loss 0.0267 StepTime 1.50s\n",
      "Epoch 3/3 Step 100/105 Loss 0.0312 StepTime 1.47s\n",
      "Epoch 3/3 Step 101/105 Loss 0.0401 StepTime 1.48s\n",
      "Epoch 3/3 Step 102/105 Loss 0.0832 StepTime 1.60s\n",
      "Epoch 3/3 Step 103/105 Loss 0.0135 StepTime 1.48s\n",
      "Epoch 3/3 Step 104/105 Loss 0.0097 StepTime 1.50s\n",
      "Epoch 3/3 Step 105/105 Loss 0.0147 StepTime 1.46s\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "the vengeance of their enemies. Immediately they ran in crowds to the\n",
      "GOLD: e-thay engeance-vay of-ay eir-thay enemies-ay. ediately-Immay ey-thay an-ray in-ay owds-cray o-tay e-thay\n",
      "PRED: e-thay engeance-vay of-ay eir-thay enemies-ay. immediately-ay ey-thay an-ray in-ay ounds-cray o-tay e-thay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "gates were left open, are crowded together in them; then the Germans\n",
      "GOLD: ates-gay ere-way eft-lay open-ay, are-ay owded-cray ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "PRED: ates-gay ere-way eft-lay open-ay, are-ay umbered-cay ogether-tay in-ay em-thay; en-thay e-thay ermans-Gay\n",
      "OK? : False\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "30 sentiments such of his fellow citizens as he has access to: he\n",
      "GOLD: 30 entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "PRED: 30 entiments-say uch-say of-ay is-hay ellow-fay itizens-cay as-ay e-hay as-hay access-ay o-tay: e-hay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "X.--Roscius and Lucius Caesar, having received this message, went to\n",
      "GOLD: oscius-X.--Ray and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "PRED: oscius-X.--Ray and-ay ucius-Lay aesar-Cay, aving-hay eceived-ray is-thay essage-may, ent-way o-tay\n",
      "OK? : True\n",
      "----\n",
      "IN  : Translate this to Pig Latin:\n",
      "border-men. Germans, G. i. 51\n",
      "GOLD: order-men-bay. ermans-Gay, G-ay. i-ay. 51\n",
      "PRED: order-men-bay. erns-Gay, G-ay. i-ay. 51\n",
      "OK? : False\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>▁</td></tr><tr><td>perf/avg_step_time_s</td><td>▁</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/est_completion_tokens_total</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/step_time_s</td><td>▁▁▁▁█▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>perf/wall_time_s</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅██████████</td></tr><tr><td>train/loss</td><td>█▆▄▄▄▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/exact_match_norm</td><td>0.36</td></tr><tr><td>perf/avg_step_time_s</td><td>1.83279</td></tr><tr><td>perf/completion_tokens_per_epoch</td><td>11191</td></tr><tr><td>perf/est_completion_tokens_total</td><td>33573</td></tr><tr><td>perf/step_time_s</td><td>1.45524</td></tr><tr><td>perf/wall_time_s</td><td>192.90679</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/loss</td><td>0.01466</td></tr><tr><td>train/step</td><td>105</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-baseline-Qwen3-4B-Instruct-2507-20251211_230009</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/50rlky0e' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/50rlky0e</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251211_230009-50rlky0e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. wall_time_s=192.9 eval_exact_match_norm=0.360\n"
     ]
    }
   ],
   "source": [
    "USE_WANDB = True\n",
    "for m, p in EXPERIMENT_MATRIX:\n",
    "    print(\"\\n\\n### RUNNING:\", m, p)\n",
    "    run_one(m, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150d96b2-ee3d-41be-8bfe-e123acc95419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tinker-Hello-World (venv)",
   "language": "python",
   "name": "tinker-hello-world"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
