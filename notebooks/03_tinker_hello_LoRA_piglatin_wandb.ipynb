{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea9df618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: baseline\n",
      "Max train rows: 300\n",
      "LoRA rank: 16\n",
      "Learning rate: 0.0001\n",
      "Num epochs: 3\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Base model for all experiments\n",
    "BASE_MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"  # change for experiments\n",
    "\n",
    "# Which dataset slice to use\n",
    "DATASET = \"sample\"  # \"sample\" or \"full\"\n",
    "\n",
    "# Simple preset configs so you can quickly compare runs in W&B\n",
    "EXPERIMENT_PRESETS = {\n",
    "    \"baseline\": {\n",
    "        # This matches the original single-config settings\n",
    "        \"max_train_rows\": 300,\n",
    "        \"lora_rank\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_epochs\": 3,\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    \"light\": {\n",
    "        # quicker / cheaper run\n",
    "        \"max_train_rows\": 150,\n",
    "        \"lora_rank\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"num_epochs\": 2,\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "    \"heavy\": {\n",
    "        # slower / more thorough run\n",
    "        \"max_train_rows\": 500,\n",
    "        \"lora_rank\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"num_epochs\": 4,\n",
    "        \"batch_size\": 8,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Pick which preset to run\n",
    "EXPERIMENT_NAME = \"light\"  # change to \"light\" or \"heavy\" for other runs\n",
    "\n",
    "cfg = EXPERIMENT_PRESETS[EXPERIMENT_NAME]\n",
    "\n",
    "MAX_TRAIN_ROWS = cfg[\"max_train_rows\"]\n",
    "LORA_RANK      = cfg[\"lora_rank\"]\n",
    "LORA_ALPHA     = cfg[\"lora_alpha\"]\n",
    "LEARNING_RATE  = cfg[\"learning_rate\"]\n",
    "NUM_EPOCHS     = cfg[\"num_epochs\"]\n",
    "BATCH_SIZE     = cfg[\"batch_size\"]\n",
    "\n",
    "# Tag used in logging / W&B run name\n",
    "RUN_TAG = f\"piglatin-{EXPERIMENT_NAME}\"\n",
    "\n",
    "# Short model name + timestamp are used for W&B run names\n",
    "MODEL_SHORT_NAME = BASE_MODEL.split(\"/\")[-1]\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)\n",
    "print(\"Max train rows:\", MAX_TRAIN_ROWS)\n",
    "print(\"LoRA rank:\", LORA_RANK)\n",
    "print(\"Learning rate:\", LEARNING_RATE)\n",
    "print(\"Num epochs:\", NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f23bec3-51e8-403a-b8d7-7a6120d5b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: baseline\n",
      "W&B run name: piglatin-baseline-Qwen3-4B-Instruct-2507-20251201_212320\n"
     ]
    }
   ],
   "source": [
    "# --- Weights & Biases (W&B) experiment config ---\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Turn W&B tracking on/off for this run\n",
    "USE_WANDB = True  # set to False if you want to disable logging temporarily\n",
    "\n",
    "# These identify where the run shows up in your W&B account\n",
    "WANDB_PROJECT = \"tinker-hello-world\"  # you can rename later if you want\n",
    "WANDB_ENTITY = None  # or \"your-wandb-username\" if you're using a team/org\n",
    "\n",
    "# How W&B should behave\n",
    "WANDB_MODE = \"online\"  # change to \"offline\" if you don't have internet\n",
    "\n",
    "# Build a readable, unique run name like:\n",
    "# piglatin-baseline-Qwen3-4B-Instruct-2507-20251201_153000\n",
    "# (MODEL_SHORT_NAME and TIMESTAMP come from the first config cell)\n",
    "RUN_NAME = f\"{RUN_TAG}-{MODEL_SHORT_NAME}-{TIMESTAMP}\"\n",
    "\n",
    "print(\"Experiment:\", EXPERIMENT_NAME)\n",
    "print(\"W&B run name:\", RUN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff27cbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\n",
      "DATA_PATH: C:\\Users\\user\\Desktop\\tinker-hello-world\\data\\piglatin\\sample.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start=None):\n",
    "    p = Path(start or Path.cwd()).resolve()\n",
    "    for parent in [p, *p.parents]:\n",
    "        if (parent / \"pyproject.toml\").exists() or (parent / \".git\").exists():\n",
    "            return parent\n",
    "    return p\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "\n",
    "data_rel = \"data/piglatin/sample.jsonl\" if DATASET == \"sample\" else \"data/piglatin/full/piglatin.jsonl\"\n",
    "DATA_PATH = REPO_ROOT / data_rel\n",
    "\n",
    "assert DATA_PATH.exists(), f\"Missing dataset at {DATA_PATH}. Run: python scripts/build_piglatin_dataset.py\"\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"DATA_PATH:\", DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45603f",
   "metadata": {},
   "source": [
    "\n",
    "# Notebook 03 — Hello, Tinker LoRA\n",
    "\n",
    "**Objective:** Fine-tune a base model using Tinker's managed LoRA training API.\n",
    "\n",
    "**Prerequisites:** Completed `00_check_env.ipynb`, active `TINKER_API_KEY`, and access to the Tinker service.\n",
    "\n",
    "**Estimated run time:** ~5 minutes to configure plus however long your training job runs server-side.\n",
    "\n",
    "### Before you start\n",
    "1. Confirm `python test_env.py` prints `True` for the API key.\n",
    "2. Run the environment check notebook whenever you set up a new machine or virtual environment.\n",
    "3. In this notebook you'll load your credentials, inspect available models, pick a base model, and initialize a LoRA training client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceb93a52-b45c-46da-83ad-6a469495be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TINKER_API_KEY loaded: True\n",
      "Tinker SDK version: 0.3.0\n",
      "Transformers version: 4.57.1\n"
     ]
    }
   ],
   "source": [
    "# Load credentials and confirm package versions before proceeding.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tinker\n",
    "import transformers\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"TINKER_API_KEY loaded:\", bool(os.getenv(\"TINKER_API_KEY\")))\n",
    "print(\"Tinker SDK version:\", tinker.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f77d29e-bf5f-4450-bde5-8f67d373aedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported models:\n",
      "- deepseek-ai/DeepSeek-V3.1\n",
      "- deepseek-ai/DeepSeek-V3.1-Base\n",
      "- meta-llama/Llama-3.1-70B\n",
      "- meta-llama/Llama-3.1-8B\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.2-1B\n",
      "- meta-llama/Llama-3.2-3B\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- Qwen/Qwen3-235B-A22B-Instruct-2507\n",
      "- Qwen/Qwen3-30B-A3B\n",
      "- Qwen/Qwen3-30B-A3B-Base\n",
      "- Qwen/Qwen3-30B-A3B-Instruct-2507\n",
      "- Qwen/Qwen3-32B\n",
      "- Qwen/Qwen3-4B-Instruct-2507\n",
      "- Qwen/Qwen3-8B\n",
      "- Qwen/Qwen3-8B-Base\n",
      "- openai/gpt-oss-120b\n",
      "- openai/gpt-oss-20b\n"
     ]
    }
   ],
   "source": [
    "# Inspect which base models are available to your account.\n",
    "# Tinker reads TINKER_API_KEY from the environment\n",
    "service_client = tinker.ServiceClient()\n",
    "\n",
    "caps = service_client.get_server_capabilities()\n",
    "print(\"Supported models:\")\n",
    "for m in caps.supported_models:\n",
    "    print(\"-\", m.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a709e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded examples: 300\n",
      "Example input:\n",
      " Translate this to Pig Latin:\n",
      "Having assembled the senate, he reminded them of the injustice of his ...\n",
      "Example output:\n",
      " aving-Hay assembled-ay e-thay enate-say, e-hay eminded-ray em-thay of-ay e-thay injustice-ay of-ay is-hay ...\n"
     ]
    }
   ],
   "source": [
    "from tinker_hw.utils.io import read_jsonl\n",
    "\n",
    "examples = read_jsonl(DATA_PATH)[:MAX_TRAIN_ROWS]\n",
    "\n",
    "# sanity checks (fail fast)\n",
    "assert len(examples) > 0, \"No examples loaded — check DATA_PATH\"\n",
    "assert all((\"input\" in e and \"output\" in e) for e in examples), \"Expected keys: input/output\"\n",
    "\n",
    "print(\"Loaded examples:\", len(examples))\n",
    "print(\"Example input:\\n\", examples[0][\"input\"][:200], \"...\")\n",
    "print(\"Example output:\\n\", examples[0][\"output\"][:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbcacc3-dace-4ce5-a0d0-c8fa3b10a680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using base model: Qwen/Qwen3-4B-Instruct-2507\n"
     ]
    }
   ],
   "source": [
    "# Default to a lightweight 3B model for fast, low-cost demos\n",
    "print(\"Using base model:\", BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f857d47-1574-4159-9f09-5119bee0d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: nick99 (itprodirect) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\user\\Desktop\\tinker-hello-world\\notebooks\\wandb\\run-20251201_212435-g9erv36q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/g9erv36q' target=\"_blank\">piglatin-baseline-Qwen3-4B-Instruct-2507-20251201_212320</a></strong> to <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/g9erv36q' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/g9erv36q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run initialized.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize W&B run (optional) ---\n",
    "\n",
    "wandb_run = None\n",
    "\n",
    "if USE_WANDB:\n",
    "    wandb_run = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        entity=WANDB_ENTITY,\n",
    "        name=RUN_NAME,      # piglatin-baseline / -light / -heavy + model + timestamp\n",
    "        mode=WANDB_MODE,\n",
    "        config={\n",
    "            # High-level experiment info\n",
    "            \"experiment_name\": EXPERIMENT_NAME,\n",
    "            \"base_model\": BASE_MODEL,\n",
    "            \"dataset\": DATASET,\n",
    "            \"data_path\": DATA_PATH,\n",
    "            # Training data / size\n",
    "            \"max_train_rows\": MAX_TRAIN_ROWS,\n",
    "            # LoRA hyperparameters\n",
    "            \"lora_rank\": LORA_RANK,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            # Optimization hyperparameters\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Log one example pair so you can always see what the dataset looked like\n",
    "    if examples:\n",
    "        wandb.log({\n",
    "            \"example_input\": examples[0][\"input\"],\n",
    "            \"example_output\": examples[0][\"output\"],\n",
    "        })\n",
    "\n",
    "    print(\"W&B run initialized.\")\n",
    "else:\n",
    "    print(\"W&B disabled for this run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5fe4dcc-384f-44f0-b8ff-588983bc0627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 151643\n"
     ]
    }
   ],
   "source": [
    "# Spin up a managed LoRA training client. Adjust rank/params as you explore.\n",
    "\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=BASE_MODEL,\n",
    "    rank=LORA_RANK,\n",
    ")\n",
    "\n",
    "tokenizer = training_client.get_tokenizer()\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06aa0c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 300 training examples.\n",
      "Training for 3 epochs (38 steps/epoch, ~114 steps total).\n",
      "Epoch 1/3 Step 1/114 Loss 2.6197\n",
      "Epoch 1/3 Step 2/114 Loss 2.1540\n",
      "Epoch 1/3 Step 3/114 Loss 1.9037\n",
      "Epoch 1/3 Step 4/114 Loss 1.7491\n",
      "Epoch 1/3 Step 5/114 Loss 1.2676\n",
      "Epoch 1/3 Step 6/114 Loss 1.2247\n",
      "Epoch 1/3 Step 7/114 Loss 1.3471\n",
      "Epoch 1/3 Step 8/114 Loss 0.9969\n",
      "Epoch 1/3 Step 9/114 Loss 0.9919\n",
      "Epoch 1/3 Step 10/114 Loss 0.8339\n",
      "Epoch 1/3 Step 11/114 Loss 0.6095\n",
      "Epoch 1/3 Step 12/114 Loss 0.4277\n",
      "Epoch 1/3 Step 13/114 Loss 0.4885\n",
      "Epoch 1/3 Step 14/114 Loss 0.4580\n",
      "Epoch 1/3 Step 15/114 Loss 0.5287\n",
      "Epoch 1/3 Step 16/114 Loss 0.6191\n",
      "Epoch 1/3 Step 17/114 Loss 0.4734\n",
      "Epoch 1/3 Step 18/114 Loss 0.5830\n",
      "Epoch 1/3 Step 19/114 Loss 0.5787\n",
      "Epoch 1/3 Step 20/114 Loss 0.2702\n",
      "Epoch 1/3 Step 21/114 Loss 0.2716\n",
      "Epoch 1/3 Step 22/114 Loss 0.3086\n",
      "Epoch 1/3 Step 23/114 Loss 0.2873\n",
      "Epoch 1/3 Step 24/114 Loss 0.2887\n",
      "Epoch 1/3 Step 25/114 Loss 0.1840\n",
      "Epoch 1/3 Step 26/114 Loss 0.2183\n",
      "Epoch 1/3 Step 27/114 Loss 0.2019\n",
      "Epoch 1/3 Step 28/114 Loss 0.2251\n",
      "Epoch 1/3 Step 29/114 Loss 0.1942\n",
      "Epoch 1/3 Step 30/114 Loss 0.2382\n",
      "Epoch 1/3 Step 31/114 Loss 0.1378\n",
      "Epoch 1/3 Step 32/114 Loss 0.1546\n",
      "Epoch 1/3 Step 33/114 Loss 0.1570\n",
      "Epoch 1/3 Step 34/114 Loss 0.1922\n",
      "Epoch 1/3 Step 35/114 Loss 0.1557\n",
      "Epoch 1/3 Step 36/114 Loss 0.1677\n",
      "Epoch 1/3 Step 37/114 Loss 0.2093\n",
      "Epoch 1/3 Step 38/114 Loss 0.1705\n",
      "Epoch 2/3 Step 39/114 Loss 0.1843\n",
      "Epoch 2/3 Step 40/114 Loss 0.1267\n",
      "Epoch 2/3 Step 41/114 Loss 0.1158\n",
      "Epoch 2/3 Step 42/114 Loss 0.0754\n",
      "Epoch 2/3 Step 43/114 Loss 0.1108\n",
      "Epoch 2/3 Step 44/114 Loss 0.1041\n",
      "Epoch 2/3 Step 45/114 Loss 0.0850\n",
      "Epoch 2/3 Step 46/114 Loss 0.1145\n",
      "Epoch 2/3 Step 47/114 Loss 0.1556\n",
      "Epoch 2/3 Step 48/114 Loss 0.0531\n",
      "Epoch 2/3 Step 49/114 Loss 0.0878\n",
      "Epoch 2/3 Step 50/114 Loss 0.0602\n",
      "Epoch 2/3 Step 51/114 Loss 0.1885\n",
      "Epoch 2/3 Step 52/114 Loss 0.1284\n",
      "Epoch 2/3 Step 53/114 Loss 0.0710\n",
      "Epoch 2/3 Step 54/114 Loss 0.0738\n",
      "Epoch 2/3 Step 55/114 Loss 0.0411\n",
      "Epoch 2/3 Step 56/114 Loss 0.0706\n",
      "Epoch 2/3 Step 57/114 Loss 0.0599\n",
      "Epoch 2/3 Step 58/114 Loss 0.1450\n",
      "Epoch 2/3 Step 59/114 Loss 0.1198\n",
      "Epoch 2/3 Step 60/114 Loss 0.0283\n",
      "Epoch 2/3 Step 61/114 Loss 0.0410\n",
      "Epoch 2/3 Step 62/114 Loss 0.0573\n",
      "Epoch 2/3 Step 63/114 Loss 0.1662\n",
      "Epoch 2/3 Step 64/114 Loss 0.1856\n",
      "Epoch 2/3 Step 65/114 Loss 0.0830\n",
      "Epoch 2/3 Step 66/114 Loss 0.0686\n",
      "Epoch 2/3 Step 67/114 Loss 0.0956\n",
      "Epoch 2/3 Step 68/114 Loss 0.0424\n",
      "Epoch 2/3 Step 69/114 Loss 0.1734\n",
      "Epoch 2/3 Step 70/114 Loss 0.0562\n",
      "Epoch 2/3 Step 71/114 Loss 0.0324\n",
      "Epoch 2/3 Step 72/114 Loss 0.0335\n",
      "Epoch 2/3 Step 73/114 Loss 0.0794\n",
      "Epoch 2/3 Step 74/114 Loss 0.0365\n",
      "Epoch 2/3 Step 75/114 Loss 0.0223\n",
      "Epoch 2/3 Step 76/114 Loss 0.0221\n",
      "Epoch 3/3 Step 77/114 Loss 0.0401\n",
      "Epoch 3/3 Step 78/114 Loss 0.0359\n",
      "Epoch 3/3 Step 79/114 Loss 0.0215\n",
      "Epoch 3/3 Step 80/114 Loss 0.0252\n",
      "Epoch 3/3 Step 81/114 Loss 0.0273\n",
      "Epoch 3/3 Step 82/114 Loss 0.0452\n",
      "Epoch 3/3 Step 83/114 Loss 0.0271\n",
      "Epoch 3/3 Step 84/114 Loss 0.0726\n",
      "Epoch 3/3 Step 85/114 Loss 0.0387\n",
      "Epoch 3/3 Step 86/114 Loss 0.0246\n",
      "Epoch 3/3 Step 87/114 Loss 0.0101\n",
      "Epoch 3/3 Step 88/114 Loss 0.0457\n",
      "Epoch 3/3 Step 89/114 Loss 0.0417\n",
      "Epoch 3/3 Step 90/114 Loss 0.0210\n",
      "Epoch 3/3 Step 91/114 Loss 0.0235\n",
      "Epoch 3/3 Step 92/114 Loss 0.0124\n",
      "Epoch 3/3 Step 93/114 Loss 0.0293\n",
      "Epoch 3/3 Step 94/114 Loss 0.0178\n",
      "Epoch 3/3 Step 95/114 Loss 0.0267\n",
      "Epoch 3/3 Step 96/114 Loss 0.0107\n",
      "Epoch 3/3 Step 97/114 Loss 0.0185\n",
      "Epoch 3/3 Step 98/114 Loss 0.0062\n",
      "Epoch 3/3 Step 99/114 Loss 0.0171\n",
      "Epoch 3/3 Step 100/114 Loss 0.0056\n",
      "Epoch 3/3 Step 101/114 Loss 0.0643\n",
      "Epoch 3/3 Step 102/114 Loss 0.0253\n",
      "Epoch 3/3 Step 103/114 Loss 0.0373\n",
      "Epoch 3/3 Step 104/114 Loss 0.0214\n",
      "Epoch 3/3 Step 105/114 Loss 0.0095\n",
      "Epoch 3/3 Step 106/114 Loss 0.0314\n",
      "Epoch 3/3 Step 107/114 Loss 0.0277\n",
      "Epoch 3/3 Step 108/114 Loss 0.0330\n",
      "Epoch 3/3 Step 109/114 Loss 0.0144\n",
      "Epoch 3/3 Step 110/114 Loss 0.0132\n",
      "Epoch 3/3 Step 111/114 Loss 0.0106\n",
      "Epoch 3/3 Step 112/114 Loss 0.0279\n",
      "Epoch 3/3 Step 113/114 Loss 0.0150\n",
      "Epoch 3/3 Step 114/114 Loss 0.0069\n",
      "Training loop finished.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▂▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/num_examples</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>example_input</td><td>Translate this to Pi...</td></tr><tr><td>example_output</td><td>aving-Hay assembled-...</td></tr><tr><td>train/batch_size</td><td>4</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/loss</td><td>0.00686</td></tr><tr><td>train/num_examples</td><td>300</td></tr><tr><td>train/step</td><td>114</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">piglatin-baseline-Qwen3-4B-Instruct-2507-20251201_212320</strong> at: <a href='https://wandb.ai/itprodirect/tinker-hello-world/runs/g9erv36q' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world/runs/g9erv36q</a><br> View project at: <a href='https://wandb.ai/itprodirect/tinker-hello-world' target=\"_blank\">https://wandb.ai/itprodirect/tinker-hello-world</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251201_212435-g9erv36q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B run finished.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tinker import types\n",
    "\n",
    "# 1) Convert raw examples -> Tinker Datum objects (once per run)\n",
    "\n",
    "def process_example(example: dict, tokenizer) -> types.Datum:\n",
    "    # Format input/output the same way as the Tinker docs\n",
    "    prompt = f\"English: {example['input']}\\nPig Latin:\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    prompt_weights = [0] * len(prompt_tokens)\n",
    "\n",
    "    # Space before output, end with double newline\n",
    "    completion_tokens = tokenizer.encode(\n",
    "        f\" {example['output']}\\n\\n\", add_special_tokens=False\n",
    "    )\n",
    "    completion_weights = [1] * len(completion_tokens)\n",
    "\n",
    "    tokens = prompt_tokens + completion_tokens\n",
    "    weights = prompt_weights + completion_weights\n",
    "\n",
    "    # Shift for next-token prediction\n",
    "    input_tokens = tokens[:-1]\n",
    "    target_tokens = tokens[1:]\n",
    "    weights = weights[1:]\n",
    "\n",
    "    return types.Datum(\n",
    "        model_input=types.ModelInput.from_ints(tokens=input_tokens),\n",
    "        loss_fn_inputs=dict(\n",
    "            weights=weights,\n",
    "            target_tokens=target_tokens,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "processed_examples = [process_example(ex, tokenizer) for ex in examples]\n",
    "print(f\"Prepared {len(processed_examples)} training examples.\")\n",
    "\n",
    "# 2) Training loop: multiple epochs over the small dataset\n",
    "\n",
    "num_examples = len(processed_examples)\n",
    "steps_per_epoch = int(np.ceil(num_examples / BATCH_SIZE))\n",
    "total_steps = NUM_EPOCHS * steps_per_epoch\n",
    "\n",
    "print(\n",
    "    f\"Training for {NUM_EPOCHS} epochs \"\n",
    "    f\"({steps_per_epoch} steps/epoch, ~{total_steps} steps total).\"\n",
    ")\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Shuffle each epoch to avoid learning order-specific quirks\n",
    "    random.shuffle(processed_examples)\n",
    "\n",
    "    for start in range(0, num_examples, BATCH_SIZE):\n",
    "        batch = processed_examples[start:start + BATCH_SIZE]\n",
    "\n",
    "        # Queue forward+backward and optimizer step on Tinker\n",
    "        fwdbwd_future = training_client.forward_backward(\n",
    "            batch,\n",
    "            loss_fn=\"cross_entropy\",\n",
    "        )\n",
    "        optim_future = training_client.optim_step(\n",
    "            types.AdamParams(learning_rate=LEARNING_RATE)\n",
    "        )\n",
    "\n",
    "        # Wait for results so we can compute a scalar loss\n",
    "        fwdbwd_result = fwdbwd_future.result()\n",
    "        optim_future.result()\n",
    "\n",
    "        # 3) Compute weighted token-level loss, same as Tinker docs\n",
    "        logprobs = np.concatenate(\n",
    "            [out[\"logprobs\"].tolist() for out in fwdbwd_result.loss_fn_outputs]\n",
    "        )\n",
    "        weights = np.concatenate(\n",
    "            [ex.loss_fn_inputs[\"weights\"].tolist() for ex in batch]\n",
    "        )\n",
    "\n",
    "        loss = -float(np.dot(logprobs, weights) / weights.sum())\n",
    "        global_step += 1\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{NUM_EPOCHS} \"\n",
    "            f\"Step {global_step}/{total_steps} \"\n",
    "            f\"Loss {loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 4) Log to W&B, if enabled\n",
    "        if USE_WANDB and wandb_run is not None:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"train/loss\": loss,\n",
    "                    \"train/epoch\": epoch + 1,\n",
    "                    \"train/step\": global_step,\n",
    "                    \"train/batch_size\": len(batch),\n",
    "                    \"train/num_examples\": num_examples,\n",
    "                },\n",
    "                step=global_step,\n",
    "            )\n",
    "\n",
    "print(\"Training loop finished.\")\n",
    "\n",
    "# If W&B is enabled, mark the run as finished\n",
    "if USE_WANDB and wandb_run is not None:\n",
    "    try:\n",
    "        wandb.finish()\n",
    "        print(\"W&B run finished.\")\n",
    "    except Exception as e:\n",
    "        print(\"W&B finish() failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32fa5043-1250-4b7a-9dca-29867071af52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : coffee break\n",
      "OUTPUT: ' offee-cay eak-bray\\n\\n'\n",
      "INPUT : I am learning to fine tune LLMs\n",
      "OUTPUT: ' am-I aring-to-ay LLMs-lay\\n\\n'\n",
      "INPUT : Basketball is my favorite sport\n",
      "OUTPUT: ' asketball-ay is-may is-favorite-say ortball-bay\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "from tinker import types\n",
    "\n",
    "# 1) Snapshot weights and get a sampling client\n",
    "sampling_client = training_client.save_weights_and_get_sampling_client(\n",
    "    name=RUN_NAME  # ties weights to this specific experiment/run\n",
    ")\n",
    "\n",
    "def piglatin_demo(text: str):\n",
    "    prompt = f\"English: {text}\\nPig Latin:\"\n",
    "    # Be explicit and mirror training-time tokenization\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "    model_input = types.ModelInput.from_ints(prompt_tokens)\n",
    "\n",
    "    params = types.SamplingParams(\n",
    "        max_tokens=40,\n",
    "        temperature=0.1,\n",
    "        stop=[\"\\n\"],\n",
    "    )\n",
    "\n",
    "    future = sampling_client.sample(\n",
    "        prompt=model_input,\n",
    "        sampling_params=params,\n",
    "        num_samples=1,\n",
    "    )\n",
    "    result = future.result()\n",
    "    decoded = tokenizer.decode(result.sequences[0].tokens)\n",
    "\n",
    "    print(\"INPUT :\", text)\n",
    "    print(\"OUTPUT:\", repr(decoded))\n",
    "\n",
    "# Try a few\n",
    "piglatin_demo(\"coffee break\")\n",
    "piglatin_demo(\"I am learning to fine tune LLMs\")\n",
    "piglatin_demo(\"Basketball is my favorite sport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d252f726-01fb-4c09-855d-45611c7d5958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tinker-Hello-World (venv)",
   "language": "python",
   "name": "tinker-hello-world"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
